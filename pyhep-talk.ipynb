{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4322f8a5-29d3-4d9a-afaf-f532ff06e246",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# <br> Up your ML game \n",
    "![title](images/levelup.jpg)\n",
    "## PyHEP  \n",
    "### Liv V√•ge 28.10.2025 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02b7542",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Why this talk?\n",
    "\n",
    "The ML ecosystem changes **fast**. \n",
    "\n",
    "E.g. by the time you finally understand the intricacies of one package it falls out of favour (_tensorflow_ üëÄ)\n",
    "\n",
    "**Goal:** Help you pick the right tools, increase efficiency and smooth out pain points \n",
    "\n",
    "**Disclaimer:** If you're well versed in ML, there might not be a lot of new material here. And these are just my biased opinions -\n",
    "please make a PR with edits if you find mistakes or have something to add! \n",
    "\n",
    "---\n",
    "\n",
    "## What we'll cover:\n",
    "1. **ML Frameworks** - PyTorch, JAX, Keras, etc. (which one and why?)\n",
    "2. **Workflow Tools** - W&B, MLflow, Optuna (making it trackable and reproducable)\n",
    "3. **Training & Deployment** - hls4ml, ONNX, HTCondor (get off your laptop)\n",
    "4. **HEP-ML Bridge** - uproot, awkward, hist (one of the major pain points)\n",
    "5. **Industry Tools** - What industry does better (and what we can steal)\n",
    "6. **Fun Shortcuts** - LLMs, Hugging Face, and other \"cheats\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624b1428",
   "metadata": {},
   "source": [
    "# 1. Common ML Frameworks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fa4ca5-3ac3-4f48-ab73-c3fba7f1c676",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "## Which framework should I use?\n",
    "\n",
    "**Short answer:** PyTorch (probably)\n",
    "\n",
    "**Long answer:** Depends on the use case "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b85136",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Quick Framework Comparison\n",
    "\n",
    "| Framework | Best For | Pros | Cons |\n",
    "|-----------|----------|------|------|\n",
    "| **PyTorch** | Research, flexibility, HEP | Pythonic, great debugging, huge community | Verbose, more boilerplate |\n",
    "| **PyTorch Lightning** | Production, clean code | Organized, less boilerplate, built-in best practices | Another abstraction to learn |\n",
    "| **JAX** | Speed demons, researchers | FAST, functional programming, auto-vectorization | NumPy 2.x conflicts, functional paradigm learning curve |\n",
    "| **Keras** | Beginners, quick prototypes | Super simple API, fast to start | Less flexibility, slower development |\n",
    "| **Scikit-learn** | Classical ML, baselines | Easy, stable, great docs | Not for deep learning |\n",
    "| **XGBoost** | Tabular data, structured features | Fast, interpretable, great for HEP kinematics | NumPy 2.x compatibility issues, not for complex deep learning |\n",
    "| **Tensorflow** | Legacy code | You might find legacy code examples in tensorflow | It's falling out of favour, would avoid if possible|\n",
    "\n",
    "_Note that Keras is actually an API that lets you call jax, tensorflow and pytorch!_ \n",
    "\n",
    "- Just starting? ‚Üí **Keras** or **Scikit-learn**\n",
    "- Need a quick baseline on tabular data? ‚Üí **XGBoost** or **Scikit-learn**\n",
    "- Working with HEP kinematic features? ‚Üí **XGBoost** (often best!)\n",
    "- Doing research/custom architectures? ‚Üí **PyTorch**\n",
    "- Want cleaner code? ‚Üí **PyTorch Lightning**\n",
    "- Need maximum speed? ‚Üí **JAX**\n",
    "- Working in a team? ‚Üí **PyTorch** or **PyTorch Lightning**\n",
    "- You can also write custom Cuda code if you really like to suffer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff39a40",
   "metadata": {},
   "source": [
    "### 1.1 - XGBoost: The Gradient Boosting Powerhouse\n",
    "\n",
    "**What it is:** Extreme Gradient Boosting - tree-based ensemble method\n",
    "\n",
    "**Why it matters for HEP:**\n",
    "- Handles tabular data exceptionally well (which HEP has lots of!)\n",
    "- Often outperforms neural networks on structured data\n",
    "- Interpretable (feature importance, SHAP values)\n",
    "- Fast training and inference\n",
    "- Great baseline before trying deep learning\n",
    "\n",
    "**When to use:**\n",
    "- Tabular data with many features\n",
    "- Need quick, interpretable results\n",
    "- Want feature importance\n",
    "- Limited training data available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d97bf35f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: (1000, 4), Labels: (1000,)\n"
     ]
    }
   ],
   "source": [
    "# Quick demo: Same simple neural network in different frameworks\n",
    "import numpy as np\n",
    "\n",
    "# Generate some fake HEP-like data (4 kinematic features)\n",
    "np.random.seed(42)\n",
    "X_train = np.random.randn(1000, 4).astype(np.float32)\n",
    "y_train = (X_train[:, 0] + X_train[:, 1] > 0).astype(np.float32)\n",
    "X_test = np.random.randn(200, 4).astype(np.float32)\n",
    "y_test = (X_test[:, 0] + X_test[:, 1] > 0).astype(np.float32)\n",
    "\n",
    "print(f\"Training data: {X_train.shape}, Labels: {y_train.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ca82cd8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Accuracy: 0.980\n",
      "Feature Importance:\n",
      "  Feature 0: 0.509\n",
      "  Feature 1: 0.462\n",
      "  Feature 2: 0.015\n",
      "  Feature 3: 0.015\n"
     ]
    }
   ],
   "source": [
    "# XGBoost example\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Train XGBoost\n",
    "clf_xgb = xgb.XGBClassifier(n_estimators=100, random_state=42, eval_metric='logloss', verbosity=0)\n",
    "clf_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_xgb = clf_xgb.predict(X_test)\n",
    "acc_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "print(f\"XGBoost Accuracy: {acc_xgb:.3f}\")\n",
    "\n",
    "# Show feature importance\n",
    "print(\"Feature Importance:\")\n",
    "for i, imp in enumerate(clf_xgb.feature_importances_):\n",
    "    print(f\"  Feature {i}: {imp:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cddc9c8",
   "metadata": {},
   "source": [
    "## 1.2 - Scikit learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b8cfa8",
   "metadata": {},
   "source": [
    "Great for a range of ML models and quick benchmarking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73fd29b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Scikit-learn: 3 lines and done. Accuracy: 0.995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liv/pyhep-talk/venv-pyhep/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "\n",
    "# Match the same hyperparameters as other neural networks\n",
    "start_sklearn = time.time()\n",
    "clf = MLPClassifier(hidden_layer_sizes=(4, 16, 1), max_iter=20, random_state=42,\n",
    "                    learning_rate='constant', learning_rate_init=0.001,\n",
    "                    solver='adam', activation='tanh', batch_size=32)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = clf.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "time_sklearn = time.time() - start_sklearn\n",
    "print(f\"‚úÖ Scikit-learn: 3 lines and done. Accuracy: {acc:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2e347a",
   "metadata": {},
   "source": [
    "A very neat detail is that in most libraries (xgboost, sklearn, pytorch, keras) follow the same general pattern of \n",
    "```\n",
    "model = ...\n",
    "model.fit(data)\n",
    "model.predict(data)\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f90d7f",
   "metadata": {},
   "source": [
    "## 1.3 - Neural networks "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456bf70d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Option 1: PyTorch (the verbose way)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83af14e8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ PyTorch model defined\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Define PyTorch model class\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Standard architecture: 4 -> 16 -> 1\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(4, 16),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(16, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "print(\"‚úÖ PyTorch model defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "97a03fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ PyTorch: Full control. Accuracy: 0.980\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# PyTorch training with exact same hyperparameters\n",
    "start_pytorch = time.time()\n",
    "torch.manual_seed(42)\n",
    "model_torch = SimpleNN()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model_torch.parameters(), lr=0.001)\n",
    "\n",
    "X_train_t = torch.from_numpy(X_train).float()\n",
    "y_train_t = torch.from_numpy(y_train).float().unsqueeze(1)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_t, y_train_t)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "model_torch.train()\n",
    "for epoch in range(20):\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_torch(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "model_torch.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = (model_torch(torch.from_numpy(X_test).float()).numpy().flatten() > 0.5).astype(int)\n",
    "    acc_pytorch = accuracy_score(y_test, predictions)\n",
    "\n",
    "time_pytorch = time.time() - start_pytorch\n",
    "print(f\"‚úÖ PyTorch: Full control. Accuracy: {acc_pytorch:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6509d747",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Option 2: PyTorch Lightning (the clean and quick code way)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb4ec23",
   "metadata": {},
   "source": [
    "Pytorch lightning relies on inheritance so you don't have to write boilerplate code. It's great for development, but is generally disfavoured in production - you only need inference and other libraries handle that better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "925bb3b9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ PyTorch Lightning model class defined\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "class LitModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(42)  # Match other frameworks\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(4, 16), nn.Tanh(), nn.Linear(16, 1), nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.BCELoss()(y_hat, y)\n",
    "        self.log('train_loss', loss, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(), lr=0.001)\n",
    "\n",
    "print(\"‚úÖ PyTorch Lightning model class defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e3c0dcc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/liv/pyhep-talk/venv-pyhep/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ PyTorch Lightning: Clean code. Accuracy: 0.980\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import pytorch_lightning as pl\n",
    "import time\n",
    "\n",
    "# PyTorch Lightning training\n",
    "start_pl = time.time()\n",
    "torch.manual_seed(42)\n",
    "lit_model = LitModel()\n",
    "train_dataset = TensorDataset(X_train_t, y_train_t)\n",
    "train_loader_pl = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "trainer = pl.Trainer(max_epochs=20, log_every_n_steps=1000, enable_progress_bar=False, enable_model_summary=False, logger=False, enable_checkpointing=False, fast_dev_run=False)\n",
    "trainer.fit(lit_model, train_loader_pl)\n",
    "\n",
    "# Faster evaluation - use the underlying PyTorch model directly\n",
    "lit_model.eval()\n",
    "X_test_t = torch.from_numpy(X_test).float()\n",
    "with torch.no_grad():\n",
    "    outputs = lit_model(X_test_t)\n",
    "    predictions_pl = (outputs.squeeze().numpy() > 0.5).astype(int)\n",
    "    acc_pl = accuracy_score(y_test, predictions_pl)\n",
    "\n",
    "time_pl = time.time() - start_pl\n",
    "print(f\"‚úÖ PyTorch Lightning: Clean code. Accuracy: {acc_pl:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331ab310",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Option 3: Keras (the simple API way)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a84dc77c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liv/pyhep-talk/venv-pyhep/lib/python3.12/site-packages/keras/src/layers/core/dense.py:95: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Keras: 4 lines of code. Accuracy: 0.985\n"
     ]
    }
   ],
   "source": [
    "# Keras: Super simple API\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "# Set random seed for reproducibility\n",
    "start_keras = time.time()\n",
    "tf.random.set_seed(42)\n",
    "# Define and compile model - matches PyTorch architecture\n",
    "model_keras = keras.Sequential([\n",
    "    layers.Dense(16, activation='tanh', input_shape=(4,)),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model_keras.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "                    loss='binary_crossentropy')\n",
    "# Train\n",
    "model_keras.fit(X_train, y_train, epochs=20, batch_size=32, verbose=0)\n",
    "# Evaluate\n",
    "y_pred_keras = (model_keras.predict(X_test, verbose=0) > 0.5).astype(int).flatten()\n",
    "acc_keras = accuracy_score(y_test, y_pred_keras)\n",
    "time_keras = time.time() - start_keras\n",
    "print(f\"‚úÖ Keras: 4 lines of code. Accuracy: {acc_keras:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47452bd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Option 4: JAX (the functional way)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dcaf2a6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ JAX: Functional and fast. Accuracy: 0.960\n"
     ]
    }
   ],
   "source": [
    "# JAX: Functional programming approach\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "import optax\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import time\n",
    "# Set seed for reproducibility\n",
    "start_jax = time.time()\n",
    "key = random.PRNGKey(42)\n",
    "np.random.seed(42)\n",
    "# Initialize parameters - same architecture as others\n",
    "layer_sizes = [4, 16, 1]\n",
    "keys = random.split(key, len(layer_sizes))\n",
    "params = []\n",
    "for m, n, k in zip(layer_sizes[:-1], layer_sizes[1:], keys):\n",
    "    w_key, b_key = random.split(k)\n",
    "    w = random.normal(w_key, (m, n)) * 0.1\n",
    "    b = random.normal(b_key, (n,)) * 0.1\n",
    "    params.append((w, b))\n",
    "# Forward pass\n",
    "def forward(params, x):\n",
    "    for w, b in params[:-1]:\n",
    "        x = jnp.tanh(x @ w + b)\n",
    "    w, b = params[-1]\n",
    "    return jax.nn.sigmoid(x @ w + b).squeeze()\n",
    "# Loss function\n",
    "def loss_fn(params, x, y):\n",
    "    pred = forward(params, x)\n",
    "    return optax.sigmoid_binary_cross_entropy(pred, y).mean()\n",
    "# Training\n",
    "optimizer_jax = optax.adam(learning_rate=0.001)\n",
    "opt_state = optimizer_jax.init(params)\n",
    "X_train_jax = jnp.array(X_train)\n",
    "y_train_jax = jnp.array(y_train)\n",
    "X_test_jax = jnp.array(X_test)\n",
    "y_test_jax = jnp.array(y_test)\n",
    "batch_size = 32\n",
    "n_batches = len(X_train) // batch_size\n",
    "for epoch in range(20):\n",
    "    for batch_idx in range(n_batches):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "        X_batch = X_train_jax[start_idx:end_idx]\n",
    "        y_batch = y_train_jax[start_idx:end_idx]\n",
    "        loss, grads = jax.value_and_grad(loss_fn)(params, X_batch, y_batch)\n",
    "        updates, opt_state = optimizer_jax.update(grads, opt_state)\n",
    "        params = optax.apply_updates(params, updates)\n",
    "# Evaluate\n",
    "test_pred = forward(params, X_test_jax) > 0.5\n",
    "acc_jax = accuracy_score(y_test, np.array(test_pred))\n",
    "time_jax = time.time() - start_jax\n",
    "print(f\"‚úÖ JAX: Functional and fast. Accuracy: {acc_jax:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "230f1da6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FRAMEWORK COMPARISON\n",
      "============================================================\n",
      "All neural networks use identical setup:\n",
      "- Architecture: 4‚Üí16‚Üí1 (tanh activation)\n",
      "- Optimizer: Adam (lr=0.001)\n",
      "- Training: 20 epochs, batch_size=32\n",
      "- Random seed: 42\n",
      "============================================================\n",
      "\n",
      "Results:\n",
      "------------------------------------------------------------\n",
      "Scikit-learn:  Accuracy: 0.995,  Time: 0.118s\n",
      "PyTorch:       Accuracy: 0.980,  Time: 0.254s\n",
      "PyTorch L.:    Accuracy: 0.980,  Time: 2.026s\n",
      "Keras:         Accuracy: 0.985,  Time: 1.433s\n",
      "JAX:           Accuracy: 0.960,  Time: 8.122s\n",
      "------------------------------------------------------------\n",
      "\n",
      "‚úÖ All frameworks achieved similar accuracy (~0.98)\n",
      "üí° PyTorch Lightning is slower due to framework overhead\n",
      "üí° Pick based on ease of use, not performance differences!\n"
     ]
    }
   ],
   "source": [
    "# Comparison: Timing and Accuracy\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FRAMEWORK COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(\"All neural networks use identical setup:\")\n",
    "\n",
    "print(\"- Architecture: 4‚Üí16‚Üí1 (tanh activation)\")\n",
    "print(\"- Optimizer: Adam (lr=0.001)\")\n",
    "print(\"- Training: 20 epochs, batch_size=32\")\n",
    "print(\"- Random seed: 42\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Compare the results from individual training cells above\n",
    "print()\n",
    "print(\"Results:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Scikit-learn:  Accuracy: {acc:.3f},  Time: {time_sklearn:.3f}s\")\n",
    "print(f\"PyTorch:       Accuracy: {acc_pytorch:.3f},  Time: {time_pytorch:.3f}s\")\n",
    "print(f\"PyTorch L.:    Accuracy: {acc_pl:.3f},  Time: {time_pl:.3f}s\")\n",
    "print(f\"Keras:         Accuracy: {acc_keras:.3f},  Time: {time_keras:.3f}s\")\n",
    "print(f\"JAX:           Accuracy: {acc_jax:.3f},  Time: {time_jax:.3f}s\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "print()\n",
    "print(\"‚úÖ All frameworks achieved similar accuracy (~0.98)\")\n",
    "print(\"üí° PyTorch Lightning is slower due to framework overhead\")\n",
    "print(\"üí° Pick based on ease of use, not performance differences!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparison_summary",
   "metadata": {},
   "source": [
    "## Takeaway:\n",
    "\n",
    "All neural networks above use **identical setup**: \n",
    "- Architecture: 4‚Üí16‚Üí1 (tanh activation)\n",
    "- Optimizer: Adam (lr=0.001)\n",
    "- Training: 20 epochs\n",
    "- Batch size: 32\n",
    "- Same random seed (42)\n",
    "\n",
    "The comparison shows:\n",
    "- **Accuracy should are very similar** across neural network frameworks (differences reflect implementation details and numerical precision)\n",
    "- **Speed varies** due to different optimizations and backend implementations - also because this is a very small example \n",
    "\n",
    "**Bottom line:** Choose your framework based on ease of use and ecosystem, not tiny performance differences.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6747c527",
   "metadata": {},
   "source": [
    " # 2 - ML workflow tools "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1de4eb",
   "metadata": {},
   "source": [
    "Moving beyond jupyter notebooks and into configuration file centred and reproducible ML "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b957acc7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 2.1 Logging experiments with Weights & Biases (W&B) \n",
    "\n",
    "**What it does:**\n",
    "- Automatic logging of metrics, hyperparameters, system info\n",
    "- Beautiful dashboards\n",
    "- Experiment comparison\n",
    "- Model versioning\n",
    "- Artifact tracking\n",
    "- **Free for academics!**\n",
    "\n",
    "**When to use:** Any serious project. Very easy to set up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "61d4278a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: W&B requires account setup (wandb.ai)\n",
      "üé® W&B: Beautiful dashboards for experiment tracking\n",
      "üí° Pro tip: wandb.watch(model) tracks gradients automatically!\n",
      "üí° Free for academics!\n"
     ]
    }
   ],
   "source": [
    "# W&B Quick Start - Experiment Monitoring\n",
    "\n",
    "try:\n",
    "    import wandb\n",
    "    wandb.login(key=\"your-key-here\")  # Would need account\n",
    "    \n",
    "    # Initialize tracking\n",
    "    wandb.init(\n",
    "        project=\"pyhep-demo\", \n",
    "        name=\"neural-network-comparison\",\n",
    "        config={\n",
    "            \"learning_rate\": 0.001,\n",
    "            \"epochs\": 20,\n",
    "            \"batch_size\": 32,\n",
    "            \"architecture\": \"Scale‚Üí16‚Üí1\"\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Simulate training with fake metrics\n",
    "    for epoch in range(5):\n",
    "        fake_loss = 1.0 / (epoch + 2) + 0.1\n",
    "        fake_acc = 0.8 + epoch * 0.04\n",
    "        wandb.log({\n",
    "            \"loss\": fake_loss,\n",
    "            \"accuracy\": fake_acc,\n",
    "            \"epoch\": epoch\n",
    "        })\n",
    "    \n",
    "    wandb.finish()\n",
    "    print(\"üé® W&B: Beautiful dashboards for experiment tracking\")\n",
    "except Exception as e:\n",
    "    print(\"Note: W&B requires account setup (wandb.ai)\")\n",
    "    print(\"üé® W&B: Beautiful dashboards for experiment tracking\")\n",
    "    print(\"üí° Pro tip: wandb.watch(model) tracks gradients automatically!\")\n",
    "    print(\"üí° Free for academics!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5f73e2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 2.2 MLflow - The Open Source Alternative\n",
    "\n",
    "**Pros:**\n",
    "- Fully open source\n",
    "- Self-hosted (for the privacy-conscious)\n",
    "- Experiment tracking + model registry\n",
    "- Works with any ML library\n",
    "\n",
    "**Cons:**\n",
    "- Less pretty than W&B\n",
    "- Need to host it yourself\n",
    "- Smaller community\n",
    "\n",
    "**When to use:** You need full control, can't/won't use cloud services\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wandb_mlflow_comparison",
   "metadata": {},
   "source": [
    "### Key Difference: W&B vs MLflow\n",
    "\n",
    "**W&B (Weights & Biases):**\n",
    "- **Best at:** Experiment monitoring, visualization, hyperparameter tuning\n",
    "- Interactive dashboards, automatic logging\n",
    "- Great for research and experimentation\n",
    "- Cloud-first (free for academics)\n",
    "\n",
    "**MLflow:**\n",
    "-  **Best at:** Model registry, versioning, deployment, MLOps\n",
    "-  Model storage and retrieval\n",
    "-  Production deployment support\n",
    "-  On-premise friendly\n",
    "\n",
    "**TL;DR:** Use W&B for experiments, MLflow for production models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4145b4d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 2.3 Optuna - Hyperparameter Optimization Made Easy\n",
    "\n",
    "**Stop doing grid search in 2025!**\n",
    "\n",
    "Optuna uses smart algorithms (TPE, CMA-ES) to find good hyperparameters faster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1dfad9ee",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-27 23:49:38,718] A new study created in memory with name: no-name-cb8c72b2-c697-4b61-b262-99327ab5e3d1\n",
      "[I 2025-10-27 23:49:38,729] Trial 0 finished with value: 0.9682190592135067 and parameters: {'lr': 0.0002924856939622585}. Best is trial 0 with value: 0.9682190592135067.\n",
      "[I 2025-10-27 23:49:38,732] Trial 1 finished with value: 0.9682190592135067 and parameters: {'lr': 0.00019878565765361588}. Best is trial 0 with value: 0.9682190592135067.\n",
      "[I 2025-10-27 23:49:38,733] Trial 2 finished with value: 0.9682190592135067 and parameters: {'lr': 2.2484412418492542e-05}. Best is trial 0 with value: 0.9682190592135067.\n",
      "[I 2025-10-27 23:49:38,744] Trial 3 finished with value: 0.8688109941757362 and parameters: {'lr': 0.0015867747971138769}. Best is trial 0 with value: 0.9682190592135067.\n",
      "[I 2025-10-27 23:49:38,745] Trial 4 finished with value: 0.9682190592135067 and parameters: {'lr': 2.4973564722352153e-05}. Best is trial 0 with value: 0.9682190592135067.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'lr': 0.0002924856939622585}\n",
      "Best value: 0.968\n",
      "üéØ Optuna: Smarter than grid search, easier than manual tuning\n",
      "üí° Integrates with W&B, PyTorch Lightning, etc.\n"
     ]
    }
   ],
   "source": [
    "# Optuna example - hyperparameter tuning\n",
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    # Suggest hyperparameters\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)\n",
    "    \n",
    "    # For demo, return a mock score based on lr\n",
    "    # In reality, you'd train a model with this lr and return validation score\n",
    "    import random\n",
    "    random.seed(int(lr * 1000))\n",
    "    accuracy = random.uniform(0.85, 0.99)\n",
    "    return accuracy\n",
    "\n",
    "# Create and run study\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=5, show_progress_bar=False)\n",
    "\n",
    "print(f\"Best params: {study.best_params}\")\n",
    "print(f\"Best value: {study.best_value:.3f}\")\n",
    "\n",
    "print(\"üéØ Optuna: Smarter than grid search, easier than manual tuning\")\n",
    "print(\"üí° Integrates with W&B, PyTorch Lightning, etc.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cern_hep_tools",
   "metadata": {},
   "source": [
    "### HEP-Specific ML Workflow Tools\n",
    "\n",
    "**b-hive** - CERN's ML platform\n",
    "- üêù Built specifically for HEP researchers\n",
    "- üîó Integrates with CERN infrastructure\n",
    "- üìä Experiment tracking + job submission\n",
    "- üéì CERN users only (automatic access)\n",
    "\n",
    "**LAW** - CERN Python library\n",
    "- üìö Law workflow automation and analysis\n",
    "- üß¨ HEP-specific patterns and tools\n",
    "- üîß Task graphs, dependencies, resubmission\n",
    "- üí™ Production analysis workflows\n",
    "\n",
    "**hep-ml-templates** - Community project\n",
    "- üöÄ Starting templates for HEP ML projects\n",
    "- üèóÔ∏è Best practices and common patterns\n",
    "- üì¶ Quick start for new projects\n",
    "- ü§ù Community-driven (on GitHub)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c633b5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Workflow Tools: Quick Comparison\n",
    "\n",
    "| Tool | Best For | Setup Difficulty | Cost |\n",
    "|------|----------|------------------|------|\n",
    "| **W&B** | Everything | Easy | Free (academic) |\n",
    "| **MLflow** | On-premise, privacy | Medium | Free (self-host) |\n",
    "| **Optuna** | Hyperparameter tuning | Easy | Free |\n",
    "| **b-hive** | CMS users | Easy | Free (CERN) |\n",
    "| **hep-ml-templates** | Quick project start | Easy | Free |\n",
    "| **LAW** | CERN workflows | Medium | Free (CERN) |\n",
    "\n",
    "**Pro tip:** Use W&B + Optuna together. They integrate perfectly!\n",
    "**For CERN users:** b-hive is built-in, lower setup barrier!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74b38b8-b5f0-421b-b889-21ef300dfc03",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# 3. Model Training & Deployment\n",
    "\n",
    "## Get off your laptop!\n",
    "\n",
    "Your MacBook is crying. Let's talk about scaling up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e845930a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### HTCondor - The HEP Classic\n",
    "\n",
    "**What it is:** Distributed computing system used at CERN and beyond\n",
    "\n",
    "**Pros:**\n",
    "- Already set up at most HEP institutions\n",
    "- Handle thousands of jobs\n",
    "- Free (for you)\n",
    "\n",
    "**Cons:**\n",
    "- Not designed for ML (but works!)\n",
    "- Can be slow to start\n",
    "- Queue times vary\n",
    "\n",
    "**When to use:** You're at a HEP institution and need to run many jobs\n",
    "\n",
    "```bash\n",
    "# Example HTCondor submit file\n",
    "# universe = vanilla\n",
    "# executable = train_model.sh\n",
    "# arguments = --learning-rate 0.001\n",
    "# queue 100 # Submit 100 jobs!\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987e003e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### SWAN - CERN's Jupyter Hub\n",
    "\n",
    "**What it is:** Cloud-based Jupyter notebooks at CERN\n",
    "\n",
    "**Pros:**\n",
    "- Access to CERN data\n",
    "- Pre-configured environment\n",
    "- Spark integration\n",
    "- GPUs available\n",
    "\n",
    "**When to use:** You're at CERN and want to prototype quickly\n",
    "\n",
    "**URL:** https://swan.cern.ch/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1ac86b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### ONNX - Make your model portable\n",
    "\n",
    "**Problem:** Trained in PyTorch, but production uses TensorFlow (or C++, or...)\n",
    "\n",
    "**Solution:** ONNX (Open Neural Network Exchange)\n",
    "\n",
    "**What it does:**\n",
    "- Convert models between frameworks\n",
    "- Optimize for inference\n",
    "- Deploy anywhere (edge devices, web, etc.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e5fa78",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# ONNX Example - Export PyTorch model\n",
    "import torch.onnx\n",
    "\n",
    "# Use our trained PyTorch model\n",
    "try:\n",
    "    # Create dummy input\n",
    "    dummy_input = torch.randn(1, 4)\n",
    "    \n",
    "    # Export to ONNX\n",
    "    torch.onnx.export(model_torch, dummy_input, \"model.onnx\", verbose=False)\n",
    "    \n",
    "    # Load and test with ONNX Runtime\n",
    "    import onnxruntime as ort\n",
    "    session = ort.InferenceSession(\"model.onnx\")\n",
    "    \n",
    "    # Test inference\n",
    "    test_input = X_test[:1].astype('float32')\n",
    "    ort_inputs = {session.get_inputs()[0].name: test_input}\n",
    "    result = session.run(None, ort_inputs)\n",
    "    \n",
    "    print(\"üì¶ ONNX: Model exported and loaded successfully!\")\n",
    "    print(f\"Original prediction: {model_torch(torch.from_numpy(X_test[:1]).float()).item():.3f}\")\n",
    "    print(f\"ONNX prediction: {result[0][0][0]:.3f}\")\n",
    "    \n",
    "    print(\"\\nüéØ ONNX: Train anywhere, deploy everywhere\")\n",
    "    print(\"üéØ Especially useful for edge deployment and production\")\n",
    "except Exception as e:\n",
    "    print(f\"Note: ONNX export demo (error: {e})\")\n",
    "    print(\"üì¶ ONNX: Train anywhere, deploy everywhere\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7471a7f2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### hls4ml - ML on FPGAs\n",
    "\n",
    "**The coolest HEP-specific tool you didn't know you needed**\n",
    "\n",
    "**Problem:** You need ultra-low latency inference (< 1 microsecond) for triggers\n",
    "\n",
    "**Solution:** hls4ml converts your neural network to FPGA firmware\n",
    "\n",
    "**Use cases:**\n",
    "- LHC trigger systems\n",
    "- Real-time event selection\n",
    "- Anything requiring hardware acceleration\n",
    "\n",
    "```python\n",
    "# import hls4ml\n",
    "# config = hls4ml.utils.config_from_keras_model(model, granularity='name')\n",
    "# hls_model = hls4ml.converters.convert_from_keras_model(\n",
    "# model, hls_config=config, output_dir='my-hls-test'\n",
    "# )\n",
    "# hls_model.compile()\n",
    "```\n",
    "\n",
    "**Mind-blowing:** Your Python model ‚Üí Hardware in < 1 hour\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8fa14b-f23f-462e-bf46-a43031c639d6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# HEP-ML Bridge Tools\n",
    "\n",
    "## The \"ROOT files aren't going anywhere\" section\n",
    "\n",
    "You can't do ML without data. In HEP, that means ROOT files, weird event structures, and ragged arrays.\n",
    "\n",
    "**These tools save your sanity:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28845a9e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### uproot - Read ROOT files without ROOT\n",
    "\n",
    "**The game changer.**\n",
    "\n",
    "Before: Install ROOT, fight with Python bindings, cry \n",
    "After: `pip install uproot`, read files with pandas-like syntax\n",
    "\n",
    "**No C++ dependencies. No ROOT installation. Pure Python bliss.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41985ec8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# uproot example - create mock HEP data\n",
    "import uproot\n",
    "import numpy as np\n",
    "from hist import Hist\n",
    "\n",
    "# Create a mock ROOT file with HEP-like data\n",
    "with uproot.recreate(\"mock_data.root\") as file:\n",
    "    # Simulate jet pt distribution\n",
    "    np.random.seed(42)\n",
    "    jet_pt = np.random.exponential(50, size=1000) * np.random.uniform(0.8, 1.2, size=1000)\n",
    "    jet_eta = np.random.normal(0, 1.5, size=1000)\n",
    "    jet_phi = np.random.uniform(-np.pi, np.pi, size=1000)\n",
    "    \n",
    "    # Create branches\n",
    "    file[\"Events\"] = {\n",
    "        \"jet_pt\": jet_pt,\n",
    "        \"jet_eta\": jet_eta,\n",
    "        \"jet_phi\": jet_phi,\n",
    "    }\n",
    "\n",
    "# Now read it back with uproot\n",
    "file = uproot.open(\"mock_data.root\")\n",
    "tree = file[\"Events\"]\n",
    "\n",
    "# Get branches as arrays\n",
    "pt = tree[\"jet_pt\"].array()\n",
    "eta = tree[\"jet_eta\"].array()\n",
    "\n",
    "# Or as pandas DataFrame\n",
    "df = tree.arrays([\"jet_pt\", \"jet_eta\", \"jet_phi\"], library=\"pd\")\n",
    "\n",
    "print(\"üéâ uproot: Read ROOT-like data successfully!\")\n",
    "print(f\"Events: {len(df)}, Columns: {list(df.columns)}\")\n",
    "print(f\"\\nSample data:\\n{df.head()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a15c30",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Awkward Array - Handle Jagged Data\n",
    "\n",
    "**Problem:** HEP events have variable-length lists (jets, tracks, etc.)\n",
    "\n",
    "**Standard approach:** Pad everything, waste memory, write ugly code\n",
    "\n",
    "**Awkward Array:** Numpy for jagged/nested/variable-length data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbf10d9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import awkward as ak\n",
    "\n",
    "# Events with variable numbers of jets\n",
    "events = ak.Array([\n",
    "    {\"jets\": [{\"pt\": 50, \"eta\": 0.1}, {\"pt\": 30, \"eta\": -0.5}]},  # 2 jets\n",
    "    {\"jets\": [{\"pt\": 100, \"eta\": 1.2}]},                           # 1 jet\n",
    "    {\"jets\": [{\"pt\": 40, \"eta\": 0.3}, {\"pt\": 35, \"eta\": 0.8}, {\"pt\": 25, \"eta\": -1.0}]}  # 3 jets\n",
    "])\n",
    "\n",
    "# Operations work naturally on jagged data!\n",
    "jet_pts = events.jets.pt\n",
    "print(\"Jet pts:\", jet_pts)\n",
    "\n",
    "# Calculate things per event\n",
    "leading_jet_pt = ak.max(events.jets.pt, axis=1)\n",
    "print(\"Leading jet pt per event:\", leading_jet_pt)\n",
    "\n",
    "# Slice like numpy\n",
    "high_pt_jets = events.jets[events.jets.pt > 35]\n",
    "print(\"High-pt jets:\", high_pt_jets)\n",
    "\n",
    "print(\"\\n‚ú® Awkward: No more padding! No more for-loops!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40dbdec7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### hist - Modern Histogramming\n",
    "\n",
    "**ROOT's TH1/TH2 are... showing their age.**\n",
    "\n",
    "`hist` is a modern, Pythonic histogramming library:\n",
    "- Clean syntax\n",
    "- Integrates with numpy, awkward\n",
    "- Beautiful plotting with matplotlib/mplhep\n",
    "- Type hints, named axes, units\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5888e3ea",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Modern histogramming with hist\n",
    "from hist import Hist\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Create histogram with named axes\n",
    "np.random.seed(42)\n",
    "pt_data = np.random.exponential(50, 1000) * np.random.uniform(0.8, 1.2, 1000)\n",
    "\n",
    "h = Hist.new.Reg(50, 0, 200, name=\"pt\", label=\"$p_T$ [GeV]\").Double()\n",
    "h.fill(pt=pt_data)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "h.plot1d(ax=ax)\n",
    "ax.set_xlabel(\"Jet $p_T$ [GeV]\")\n",
    "ax.set_ylabel(\"Events\")\n",
    "ax.set_title(\"Mock Jet $p_T$ Distribution\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"jet_pt_hist.png\", dpi=100, bbox_inches='tight')\n",
    "print(\"üìä hist: Histogram created and saved!\")\n",
    "\n",
    "print(\"\\nüí° Named axes, units, better plotting. Just better.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad33d953",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The Complete HEP-ML Pipeline\n",
    "\n",
    "```python\n",
    "import uproot\n",
    "import awkward as ak\n",
    "import numpy as np\n",
    "from hist import Hist\n",
    "\n",
    "# 1. Read ROOT file\n",
    "with uproot.open(\"data.root:Events\") as tree:\n",
    " events = tree.arrays([\"jet_*\"], library=\"ak\")\n",
    "\n",
    "# 2. Process with awkward\n",
    "good_events = events[ak.num(events.jet_pt) >= 2]\n",
    "leading_jets = good_events.jet_pt[:, 0]\n",
    "\n",
    "# 3. Make histograms\n",
    "h = Hist.new.Reg(50, 0, 200, name=\"pt\").Double()\n",
    "h.fill(leading_jets)\n",
    "\n",
    "# 4. Convert to ML format\n",
    "X = ak.to_numpy(ak.pad_none(events.jet_pt, 5, clip=True))\n",
    "# Now feed to PyTorch/JAX/etc!\n",
    "```\n",
    "\n",
    "**The dream: ROOT file ‚Üí ML model in < 50 lines**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6089f22a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Industry Tools\n",
    "\n",
    "## What industry does better (and what we can steal)\n",
    "\n",
    "HEP is amazing at physics. Industry is amazing at software engineering.\n",
    "\n",
    "**Let's learn from them:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5172abf4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Testing & Linting - Stop Breaking Things\n",
    "\n",
    "**Industry:** Comprehensive tests, CI/CD, code review, linting \n",
    "**HEP:** \"It worked on my machine\" \n",
    "\n",
    "**Tools you should use:**\n",
    "\n",
    "1. **pytest** - Testing framework\n",
    "2. **black** - Code formatter (stop arguing about formatting)\n",
    "3. **ruff** - Fast linter\n",
    "4. **mypy** - Type checking\n",
    "5. **pre-commit** - Run checks before committing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b890eb7d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Quick testing example - run simple tests\n",
    "import pytest\n",
    "\n",
    "def test_model_output_shape():\n",
    "    model = SimpleNN()\n",
    "    x = torch.randn(10, 4)\n",
    "    output = model(x)\n",
    "    assert output.shape == (10, 1), \"Wrong output shape!\"\n",
    "    return True\n",
    "\n",
    "def test_model_output_range():\n",
    "    model = SimpleNN()\n",
    "    x = torch.randn(10, 4)\n",
    "    output = model(x)\n",
    "    assert torch.all(output >= 0) and torch.all(output <= 1), \"Sigmoid broken!\"\n",
    "    return True\n",
    "\n",
    "# Run tests\n",
    "print(\"Running tests...\")\n",
    "try:\n",
    "    test_model_output_shape()\n",
    "    test_model_output_range()\n",
    "    print(\"‚úÖ All tests passed!\")\n",
    "except AssertionError as e:\n",
    "    print(f\"‚ùå Test failed: {e}\")\n",
    "\n",
    "print(\"\\nüí° Pro tip: Test your preprocessing! That's where most bugs hide.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f45416b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### GitHub Actions - Automate Everything\n",
    "\n",
    "**Stop manually running tests. Let robots do it.**\n",
    "\n",
    "Example `.github/workflows/test.yml`:\n",
    "```yaml\n",
    "name: Tests\n",
    "on: [push, pull_request]\n",
    "jobs:\n",
    " test:\n",
    " runs-on: ubuntu-latest\n",
    " steps:\n",
    " - uses: actions/checkout@v3\n",
    " - uses: actions/setup-python@v4\n",
    " with:\n",
    " python-version: '3.10'\n",
    " - run: pip install -r requirements.txt\n",
    " - run: pytest\n",
    " - run: ruff check .\n",
    "```\n",
    "\n",
    "**Now every commit is automatically tested. Magic!**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629f2e4f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### AWS SageMaker - When You Need Industrial Scale\n",
    "\n",
    "**What it is:** AWS's ML platform (training, deployment, everything)\n",
    "\n",
    "**Pros:**\n",
    "- Scales to infinity\n",
    "- Managed infrastructure\n",
    "- Production-ready deployment\n",
    "- AutoML features\n",
    "\n",
    "**Cons:**\n",
    "- Costs money (sometimes a lot)\n",
    "- Learning curve\n",
    "- Vendor lock-in\n",
    "\n",
    "**When to use:** \n",
    "- You need serious scale\n",
    "- You have budget\n",
    "- Production deployment\n",
    "\n",
    "**HEP alternative:** Usually HTCondor + custom scripts (cheaper, less polished)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5473c5e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What HEP Can Learn from Industry\n",
    "\n",
    "| Practice | Industry | HEP | What to do |\n",
    "|----------|----------|-----|-----------|\n",
    "| **Testing** | Comprehensive | Sparse | Write pytest tests! |\n",
    "| **CI/CD** | GitHub Actions | Manual | Add GitHub Actions |\n",
    "| **Code Review** | Required | Optional | Make PRs mandatory |\n",
    "| **Documentation** | Detailed | \"See code\" | Write docstrings |\n",
    "| **Versioning** | Semantic | Git SHA | Use proper versions |\n",
    "| **Linting** | Enforced | What's that? | Use ruff/black |\n",
    "\n",
    "**Bottom line:** Treat your code like a product, not a script.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ec2a2b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Fun Shortcuts & \"Cheats\"\n",
    "\n",
    "## Work smarter, not harder\n",
    "\n",
    "The secret sauce. The shortcuts your supervisor doesn't want you to know about."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec254bce",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 1. Make LLMs Do Your Work\n",
    "\n",
    "**Let's be honest:** We're all using ChatGPT/Claude/Copilot\n",
    "\n",
    "**Good uses:**\n",
    "- Boilerplate code (data loaders, training loops)\n",
    "- Documentation and docstrings\n",
    "- Bug finding\n",
    "- Code explanation\n",
    "- Unit test generation\n",
    "\n",
    "**Bad uses:**\n",
    "- Novel research code (they hallucinate)\n",
    "- Critical analysis code (verify everything!)\n",
    "- Anything you don't understand\n",
    "\n",
    "**Pro tips:**\n",
    "- Be specific: \"Write a PyTorch data loader for awkward arrays\"\n",
    "- Iterate: Start simple, add complexity\n",
    "- **Always understand the code it gives you**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a37f22",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Example: Let's ask an LLM to write a custom loss function\n",
    "# Prompt: \"Write a PyTorch loss function that combines binary cross entropy with a custom regularization term\"\n",
    "\n",
    "# LLM output (cleaned up):\n",
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self, reg_weight=0.01):\n",
    "        super().__init__()\n",
    "        self.bce = nn.BCELoss()\n",
    "        self.reg_weight = reg_weight\n",
    "    \n",
    "    def forward(self, predictions, targets, model_params):\n",
    "        bce_loss = self.bce(predictions, targets)\n",
    "        # L2 regularization\n",
    "        reg_loss = sum(p.pow(2.0).sum() for p in model_params)\n",
    "        return bce_loss + self.reg_weight * reg_loss\n",
    "\n",
    "print(\"ü§ñ LLMs: Your 24/7 coding assistant\")\n",
    "print(\"‚ö†Ô∏è  But verify everything! They confidently hallucinate.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a83e18d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 2. Steal from Hugging Face\n",
    "\n",
    "**Hugging Face:** GitHub for ML models\n",
    "\n",
    "**What's there:**\n",
    "- 500,000+ pre-trained models\n",
    "- Datasets\n",
    "- Code examples\n",
    "- Entire pipelines\n",
    "\n",
    "**You can:**\n",
    "- Fine-tune existing models (faster than training from scratch)\n",
    "- Use pre-trained embeddings\n",
    "- Copy architectures\n",
    "- Download datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7335c43",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Hugging Face example - Use a pre-trained model\n",
    "from transformers import pipeline\n",
    "\n",
    "try:\n",
    "    # Use a pre-trained sentiment analysis model (small and fast)\n",
    "    classifier = pipeline(\"sentiment-analysis\", model=\"distilbert-base-uncased-finetuned-sst-2-english\")\n",
    "    \n",
    "    # Test it\n",
    "    result = classifier(\"The LHC is amazing!\")\n",
    "    print(f\"Sentiment: {result[0]['label']} (confidence: {result[0]['score']:.3f})\")\n",
    "    print(\"\\nü§ó Hugging Face: Don't reinvent the wheel, fine-tune it!\")\n",
    "except Exception as e:\n",
    "    print(f\"Note: HF model loading skipped (error: {e})\")\n",
    "    print(\"ü§ó Hugging Face: Search for 'particle physics', 'HEP', 'jet tagging'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cde11e7c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 3. Quick Prototyping Tricks\n",
    "\n",
    "**Trick 1: Use `fastai` for rapid prototyping**\n",
    "- High-level API (even simpler than Keras)\n",
    "- Best practices built-in\n",
    "- Great for quick experiments\n",
    "\n",
    "**Trick 2: `torchinfo` for model debugging**\n",
    "```python\n",
    "from torchinfo import summary\n",
    "summary(model, input_size=(1, 4))\n",
    "# Instantly see: layers, params, output shapes\n",
    "```\n",
    "\n",
    "**Trick 3: `einops` for tensor operations**\n",
    "```python\n",
    "from einops import rearrange, reduce\n",
    "# No more confusing reshapes!\n",
    "x = rearrange(x, 'b c h w -> b (c h w)')\n",
    "```\n",
    "\n",
    "**Trick 4: `timm` for vision models**\n",
    "- 1000+ pre-trained computer vision models\n",
    "- `pip install timm`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "808aec4f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Quick tricks demo\n",
    "\n",
    "# Trick: torchinfo for model summary\n",
    "from torchinfo import summary\n",
    "\n",
    "# Show summary of our PyTorch model\n",
    "print(\"Model Summary:\")\n",
    "summary(SimpleNN(), input_size=(32, 4))  # batch_size=32, features=4\n",
    "\n",
    "# Trick: Use repr to see object details\n",
    "print(\"\\nModel architecture (repr):\")\n",
    "print(SimpleNN())\n",
    "\n",
    "# Trick: Quick timing\n",
    "import time\n",
    "start = time.time()\n",
    "# ... your code ...\n",
    "print(f\"Took {time.time() - start:.3f}s\")\n",
    "\n",
    "# Better: Use %%time or %%timeit in Jupyter!\n",
    "\n",
    "print(\"\n",
    "üí° Small tricks add up to big time savings!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d053aa",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 4. Dataset Shortcuts\n",
    "\n",
    "**Don't start from scratch:**\n",
    "\n",
    "1. **Papers with Code** - Find datasets and benchmarks\n",
    "2. **Kaggle** - Tons of curated datasets\n",
    "3. **UCI ML Repository** - Classic datasets\n",
    "4. **HEP Data** - Published HEP datasets\n",
    "5. **Zenodo** - Open science data\n",
    "\n",
    "**For HEP specifically:**\n",
    "- CERN Open Data Portal\n",
    "- LHC Olympics datasets\n",
    "- Public collision data\n",
    "\n",
    "**Pro tip:** Start with a small subset! Debug on 1000 events, not 1M.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa8867b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 5. The Ultimate Shortcut List\n",
    "\n",
    "**Must-bookmark resources:**\n",
    "\n",
    " **Learning:**\n",
    "- fast.ai course (free, excellent)\n",
    "- PyTorch tutorials (official)\n",
    "- Kaggle Learn (interactive)\n",
    "- Papers with Code (implementations)\n",
    "\n",
    "üõ†Ô∏è **Tools:**\n",
    "- GitHub Copilot / Cursor (AI pair programmer)\n",
    "- Paperswithcode.com (find state-of-the-art)\n",
    "- Connected Papers (explore research)\n",
    "\n",
    "üí¨ **Community:**\n",
    "- PyHEP working group\n",
    "- Scikit-HEP GitHub\n",
    "- ML4Jets workshop materials\n",
    "- Discord/Slack ML communities\n",
    "\n",
    "üéì **HEP-specific:**\n",
    "- IML (Inter-experimental Machine Learning)\n",
    "- ML4Jets workshops\n",
    "- PyHEP workshops\n",
    "- IRIS-HEP training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99d0378",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary: Your ML Toolkit\n",
    "\n",
    "## Quick Reference Guide\n",
    "\n",
    "### For Beginners:\n",
    "1. **Start here:** Scikit-learn for classical ML, Keras for deep learning\n",
    "2. **Read data:** uproot for ROOT files\n",
    "3. **Track experiments:** W&B (free for academics!)\n",
    "4. **Learn:** fast.ai course, official PyTorch tutorials\n",
    "\n",
    "### For Intermediate Users:\n",
    "1. **Framework:** PyTorch or PyTorch Lightning\n",
    "2. **Data:** uproot + awkward array\n",
    "3. **Optimization:** Optuna\n",
    "4. **Deployment:** ONNX\n",
    "5. **Code quality:** pytest, ruff, GitHub Actions\n",
    "\n",
    "### For Advanced Users:\n",
    "1. **Speed:** JAX for compute-intensive tasks\n",
    "2. **Scale:** HTCondor or cloud (SageMaker)\n",
    "3. **Hardware:** hls4ml for FPGAs\n",
    "4. **Tools:** Custom pipelines with all the above\n",
    "\n",
    "### Universal Tips:\n",
    "- Use version control (git)\n",
    "- Write tests (pytest)\n",
    "- Log experiments (W&B/MLflow)\n",
    "- Document your code\n",
    "- Start small, scale up\n",
    "- Leverage pre-trained models (Hugging Face)\n",
    "- Use LLMs wisely (verify everything!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf736e8b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Final Thoughts\n",
    "\n",
    "## The ML landscape is vast, but you don't need to know everything\n",
    "\n",
    "**Key takeaways:**\n",
    "\n",
    "1. **Pick tools that fit YOUR needs** - Don't use fancy tools just because they're fancy\n",
    "2. **Start simple, add complexity** - Scikit-learn ‚Üí PyTorch ‚Üí JAX\n",
    "3. **Steal shamelessly** - Use pre-trained models, copy good code, ask LLMs\n",
    "4. **Automate early** - W&B, GitHub Actions, testing save time in the long run\n",
    "5. **Bridge HEP ‚Üî ML** - uproot, awkward, hist make life easier\n",
    "6. **Learn from industry** - Testing, CI/CD, code quality matter\n",
    "7. **Community is key** - PyHEP, ML4Jets, IML, Scikit-HEP\n",
    "\n",
    "---\n",
    "\n",
    "## Most important:\n",
    "\n",
    "### **The best tool is the one you'll actually use.**\n",
    "\n",
    "Perfect code that doesn't exist < Working code that's \"good enough\"\n",
    "\n",
    "---\n",
    "\n",
    "# Questions? \n",
    "\n",
    "Resources:\n",
    "- These slides: [your-repo-link]\n",
    "- Scikit-HEP: https://scikit-hep.org/\n",
    "- PyHEP: https://hepsoftwarefoundation.org/workinggroups/pyhep.html\n",
    "- My contact: [your-contact]\n",
    "\n",
    "**Now go build something cool!** \n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "venv-pyhep",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "rise": {
   "autoPlayMedia": true,
   "autolaunch": false,
   "center": false,
   "controls": true,
   "enable_chalkboard": true,
   "height": "100%",
   "hideAddressBar": true,
   "history": true,
   "keyboard": true,
   "margin": 0,
   "maxScale": 1.5,
   "minScale": 0.2,
   "previewLinks": false,
   "progress": true,
   "scroll": true,
   "slideNumber": true,
   "start_slideshow_at": "selected",
   "theme": "night",
   "transition": "slide",
   "transitionSpeed": "slow",
   "viewDistance": 3,
   "width": "100%"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
