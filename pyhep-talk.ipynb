{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4322f8a5-29d3-4d9a-afaf-f532ff06e246",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# <br> Up your ML game \n",
    "![title](images/levelup.jpg)\n",
    "## PyHEP  \n",
    "### Liv V√•ge 28.10.2025 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02b7542",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Why this talk?\n",
    "\n",
    "The ML ecosystem changes **fast**. \n",
    "\n",
    "E.g. by the time you finally understand the intricacies of one package it falls out of favour (_tensorflow_ üëÄ)\n",
    "\n",
    "**Goal:** Help you pick the right tools, increase efficiency and smooth out pain points \n",
    "\n",
    "**Disclaimer:** If you're well versed in ML, there might not be a lot of new material here. And these are just my biased opinions -\n",
    "please make a PR with edits if you find mistakes or have something to add! \n",
    "\n",
    "---\n",
    "\n",
    "## What we'll cover:\n",
    "1. **ML Frameworks** - PyTorch, JAX, Keras, etc. (which one and why?)\n",
    "2. **Workflow Tools** - W&B, MLflow, Optuna (making it trackable and reproducable)\n",
    "3. **Training & Deployment** - ONNX, HTCondor, hls4ml (scale it up)\n",
    "4. **HEP-ML Bridge** - uproot, awkward, hist (one of the major pain points)\n",
    "5. **Industry Tools** - What industry does better (and what we can steal)\n",
    "6. **Fun Shortcuts** - LLMs, Hugging Face, and other \"cheats\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "624b1428",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 1. Common ML Frameworks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86fa4ca5-3ac3-4f48-ab73-c3fba7f1c676",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "## Which framework should I use?\n",
    "\n",
    "**Short answer:** PyTorch (probably)\n",
    "\n",
    "**Long answer:** Depends on the use case "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b85136",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Quick Framework Comparison\n",
    "\n",
    "| Framework | Best For | Pros | Cons |\n",
    "|-----------|----------|------|------|\n",
    "| **PyTorch** | Research, flexibility, HEP | Pythonic, great debugging, huge community | Verbose, more boilerplate |\n",
    "| **PyTorch Lightning** | Production, clean code | Organized, less boilerplate, built-in best practices | Another abstraction to learn |\n",
    "| **JAX** | Speed demons, researchers | FAST, functional programming, auto-vectorization | functional paradigm learning curve |\n",
    "| **Keras** | Beginners, quick prototypes | Super simple API, fast to start | Less flexibility, slower development |\n",
    "| **Scikit-learn** | Classical ML, baselines | Easy, stable, great docs | Not for deep learning |\n",
    "| **XGBoost** | Tabular data, structured features | Fast, interpretable, great for HEP kinematics | Not for complex deep learning |\n",
    "| **Tensorflow** | Legacy code | You might find legacy code examples in tensorflow | It's falling out of favour, would avoid if possible|\n",
    "\n",
    "_Note that Keras is actually an API that lets you call jax, tensorflow and pytorch!_ \n",
    "\n",
    "- Just starting? ‚Üí **Keras** or **Scikit-learn**\n",
    "- Need a quick baseline on tabular data? ‚Üí **XGBoost** or **Scikit-learn**\n",
    "- Working with HEP kinematic features? ‚Üí **XGBoost** (often best!)\n",
    "- Doing research/custom architectures? ‚Üí **PyTorch**\n",
    "- Want cleaner code? ‚Üí **PyTorch Lightning**\n",
    "- Need maximum speed? ‚Üí **JAX**\n",
    "- Working in a team? ‚Üí **PyTorch** or **PyTorch Lightning**\n",
    "- You can also write custom Cuda code if you really like to suffer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff39a40",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 1.1 - XGBoost: The Gradient Boosting Powerhouse\n",
    "\n",
    "**What it is:** Extreme Gradient Boosting - tree-based ensemble method\n",
    "\n",
    "**Why it matters for HEP:**\n",
    "- Handles tabular data exceptionally well (which HEP has lots of!)\n",
    "- Often outperforms neural networks on structured data\n",
    "- Interpretable (feature importance, SHAP values)\n",
    "- Fast training and inference\n",
    "- Great baseline before trying deep learning\n",
    "\n",
    "**When to use:**\n",
    "- Tabular data with many features\n",
    "- Need quick, interpretable results\n",
    "- Want feature importance\n",
    "- Limited training data available\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d97bf35f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data: (1000, 4), Labels: (1000,)\n"
     ]
    }
   ],
   "source": [
    "# Quick demo: Same simple neural network in different frameworks\n",
    "import numpy as np\n",
    "\n",
    "# Generate some fake HEP-like data (4 kinematic features)\n",
    "np.random.seed(42)\n",
    "X_train = np.random.randn(1000, 4).astype(np.float32)\n",
    "y_train = (X_train[:, 0] + X_train[:, 1] > 0).astype(np.float32)\n",
    "X_test = np.random.randn(200, 4).astype(np.float32)\n",
    "y_test = (X_test[:, 0] + X_test[:, 1] > 0).astype(np.float32)\n",
    "\n",
    "print(f\"Training data: {X_train.shape}, Labels: {y_train.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4ca82cd8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Accuracy: 0.980\n",
      "Feature Importance:\n",
      "  Feature 0: 0.509\n",
      "  Feature 1: 0.462\n",
      "  Feature 2: 0.015\n",
      "  Feature 3: 0.015\n"
     ]
    }
   ],
   "source": [
    "# XGBoost example\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Train XGBoost\n",
    "clf_xgb = xgb.XGBClassifier(n_estimators=100, random_state=42, eval_metric='logloss', verbosity=0)\n",
    "clf_xgb.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred_xgb = clf_xgb.predict(X_test)\n",
    "acc_xgb = accuracy_score(y_test, y_pred_xgb)\n",
    "print(f\"XGBoost Accuracy: {acc_xgb:.3f}\")\n",
    "\n",
    "# Show feature importance\n",
    "print(\"Feature Importance:\")\n",
    "for i, imp in enumerate(clf_xgb.feature_importances_):\n",
    "    print(f\"  Feature {i}: {imp:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cddc9c8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 1.2 - Scikit learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b8cfa8",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Great for a range of ML models and quick benchmarking "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "73fd29b6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Scikit-learn: 3 lines and done. Accuracy: 0.995\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liv/pyhep-talk/venv-pyhep/lib/python3.12/site-packages/sklearn/neural_network/_multilayer_perceptron.py:781: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (20) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "\n",
    "# Match the same hyperparameters as other neural networks\n",
    "start_sklearn = time.time()\n",
    "clf = MLPClassifier(hidden_layer_sizes=(4, 16, 1), max_iter=20, random_state=42,\n",
    "                    learning_rate='constant', learning_rate_init=0.001,\n",
    "                    solver='adam', activation='tanh', batch_size=32)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = clf.predict(X_test)\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "time_sklearn = time.time() - start_sklearn\n",
    "print(f\"‚úÖ Scikit-learn: 3 lines and done. Accuracy: {acc:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a2e347a",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "A very neat detail is that in most libraries (xgboost, sklearn, pytorch, keras) follow the same general pattern of \n",
    "```\n",
    "model = ...\n",
    "model.fit(data)\n",
    "model.predict(data)\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f90d7f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 1.3 - Neural networks "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456bf70d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Option 1: PyTorch (the verbose way)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "83af14e8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ PyTorch model defined\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "# Define PyTorch model class\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Standard architecture: 4 -> 16 -> 1\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(4, 16),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(16, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "print(\"‚úÖ PyTorch model defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "97a03fa0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ PyTorch: Full control. Accuracy: 0.980\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# PyTorch training with exact same hyperparameters\n",
    "start_pytorch = time.time()\n",
    "torch.manual_seed(42)\n",
    "model_torch = SimpleNN()\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model_torch.parameters(), lr=0.001)\n",
    "\n",
    "X_train_t = torch.from_numpy(X_train).float()\n",
    "y_train_t = torch.from_numpy(y_train).float().unsqueeze(1)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_t, y_train_t)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "model_torch.train()\n",
    "for epoch in range(20):\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model_torch(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "model_torch.eval()\n",
    "with torch.no_grad():\n",
    "    predictions = (model_torch(torch.from_numpy(X_test).float()).numpy().flatten() > 0.5).astype(int)\n",
    "    acc_pytorch = accuracy_score(y_test, predictions)\n",
    "\n",
    "time_pytorch = time.time() - start_pytorch\n",
    "print(f\"‚úÖ PyTorch: Full control. Accuracy: {acc_pytorch:.3f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6509d747",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Option 2: PyTorch Lightning (the clean and quick code way)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb4ec23",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Pytorch lightning relies on inheritance so you don't have to write boilerplate code. It's great for development, but is generally disfavoured in production - you only need inference and other libraries handle that better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "925bb3b9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ PyTorch Lightning model class defined\n"
     ]
    }
   ],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "class LitModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(42)  # Match other frameworks\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(4, 16), nn.Tanh(), nn.Linear(16, 1), nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x, y = batch\n",
    "        y_hat = self(x)\n",
    "        loss = nn.BCELoss()(y_hat, y)\n",
    "        self.log('train_loss', loss, on_step=False, on_epoch=True)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        return optim.Adam(self.parameters(), lr=0.001)\n",
    "\n",
    "print(\"‚úÖ PyTorch Lightning model class defined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e3c0dcc2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "/Users/liv/pyhep-talk/venv-pyhep/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:433: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ PyTorch Lightning: Clean code. Accuracy: 0.980\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import pytorch_lightning as pl\n",
    "import time\n",
    "\n",
    "# PyTorch Lightning training\n",
    "start_pl = time.time()\n",
    "torch.manual_seed(42)\n",
    "lit_model = LitModel()\n",
    "train_dataset = TensorDataset(X_train_t, y_train_t)\n",
    "train_loader_pl = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "trainer = pl.Trainer(max_epochs=20, log_every_n_steps=1000, enable_progress_bar=False, enable_model_summary=False, logger=False, enable_checkpointing=False, fast_dev_run=False)\n",
    "trainer.fit(lit_model, train_loader_pl)\n",
    "\n",
    "# Faster evaluation - use the underlying PyTorch model directly\n",
    "lit_model.eval()\n",
    "X_test_t = torch.from_numpy(X_test).float()\n",
    "with torch.no_grad():\n",
    "    outputs = lit_model(X_test_t)\n",
    "    predictions_pl = (outputs.squeeze().numpy() > 0.5).astype(int)\n",
    "    acc_pl = accuracy_score(y_test, predictions_pl)\n",
    "\n",
    "time_pl = time.time() - start_pl\n",
    "print(f\"‚úÖ PyTorch Lightning: Clean code. Accuracy: {acc_pl:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331ab310",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Option 3: Keras (the simple API way)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a84dc77c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liv/pyhep-talk/venv-pyhep/lib/python3.12/site-packages/keras/src/layers/core/dense.py:95: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Keras: 4 lines of code. Accuracy: 0.990\n"
     ]
    }
   ],
   "source": [
    "# Keras: Super simple API\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "# Set random seed for reproducibility\n",
    "start_keras = time.time()\n",
    "tf.random.set_seed(42)\n",
    "# Define and compile model - matches PyTorch architecture\n",
    "model_keras = keras.Sequential([\n",
    "    layers.Dense(16, activation='tanh', input_shape=(4,)),\n",
    "    layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model_keras.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "                    loss='binary_crossentropy')\n",
    "# Train\n",
    "model_keras.fit(X_train, y_train, epochs=20, batch_size=32, verbose=0)\n",
    "# Evaluate\n",
    "y_pred_keras = (model_keras.predict(X_test, verbose=0) > 0.5).astype(int).flatten()\n",
    "acc_keras = accuracy_score(y_test, y_pred_keras)\n",
    "time_keras = time.time() - start_keras\n",
    "print(f\"‚úÖ Keras: 4 lines of code. Accuracy: {acc_keras:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d47452bd",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Option 4: JAX (the functional way)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "dcaf2a6f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ JAX: Functional and fast. Accuracy: 0.960\n"
     ]
    }
   ],
   "source": [
    "# JAX: Functional programming approach\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import random\n",
    "import optax\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import time\n",
    "# Set seed for reproducibility\n",
    "start_jax = time.time()\n",
    "key = random.PRNGKey(42)\n",
    "np.random.seed(42)\n",
    "# Initialize parameters - same architecture as others\n",
    "layer_sizes = [4, 16, 1]\n",
    "keys = random.split(key, len(layer_sizes))\n",
    "params = []\n",
    "for m, n, k in zip(layer_sizes[:-1], layer_sizes[1:], keys):\n",
    "    w_key, b_key = random.split(k)\n",
    "    w = random.normal(w_key, (m, n)) * 0.1\n",
    "    b = random.normal(b_key, (n,)) * 0.1\n",
    "    params.append((w, b))\n",
    "# Forward pass\n",
    "def forward(params, x):\n",
    "    for w, b in params[:-1]:\n",
    "        x = jnp.tanh(x @ w + b)\n",
    "    w, b = params[-1]\n",
    "    return jax.nn.sigmoid(x @ w + b).squeeze()\n",
    "# Loss function\n",
    "def loss_fn(params, x, y):\n",
    "    pred = forward(params, x)\n",
    "    return optax.sigmoid_binary_cross_entropy(pred, y).mean()\n",
    "# Training\n",
    "optimizer_jax = optax.adam(learning_rate=0.001)\n",
    "opt_state = optimizer_jax.init(params)\n",
    "X_train_jax = jnp.array(X_train)\n",
    "y_train_jax = jnp.array(y_train)\n",
    "X_test_jax = jnp.array(X_test)\n",
    "y_test_jax = jnp.array(y_test)\n",
    "batch_size = 32\n",
    "n_batches = len(X_train) // batch_size\n",
    "for epoch in range(20):\n",
    "    for batch_idx in range(n_batches):\n",
    "        start_idx = batch_idx * batch_size\n",
    "        end_idx = start_idx + batch_size\n",
    "        X_batch = X_train_jax[start_idx:end_idx]\n",
    "        y_batch = y_train_jax[start_idx:end_idx]\n",
    "        loss, grads = jax.value_and_grad(loss_fn)(params, X_batch, y_batch)\n",
    "        updates, opt_state = optimizer_jax.update(grads, opt_state)\n",
    "        params = optax.apply_updates(params, updates)\n",
    "# Evaluate\n",
    "test_pred = forward(params, X_test_jax) > 0.5\n",
    "acc_jax = accuracy_score(y_test, np.array(test_pred))\n",
    "time_jax = time.time() - start_jax\n",
    "print(f\"‚úÖ JAX: Functional and fast. Accuracy: {acc_jax:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "230f1da6",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "FRAMEWORK COMPARISON\n",
      "============================================================\n",
      "All neural networks use identical setup:\n",
      "- Architecture: 4‚Üí16‚Üí1 (tanh activation)\n",
      "- Optimizer: Adam (lr=0.001)\n",
      "- Training: 20 epochs, batch_size=32\n",
      "- Random seed: 42\n",
      "============================================================\n",
      "\n",
      "Results:\n",
      "------------------------------------------------------------\n",
      "Scikit-learn:  Accuracy: 0.995,  Time: 0.525s\n",
      "PyTorch:       Accuracy: 0.980,  Time: 0.522s\n",
      "PyTorch L.:    Accuracy: 0.980,  Time: 2.343s\n",
      "Keras:         Accuracy: 0.990,  Time: 1.539s\n",
      "JAX:           Accuracy: 0.960,  Time: 5.259s\n",
      "------------------------------------------------------------\n",
      "\n",
      "‚úÖ All frameworks achieved similar accuracy (~0.98)\n",
      "üí° PyTorch Lightning is slower due to framework overhead\n",
      "üí° Pick based on ease of use, not performance differences!\n"
     ]
    }
   ],
   "source": [
    "# Comparison: Timing and Accuracy\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"FRAMEWORK COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(\"All neural networks use identical setup:\")\n",
    "\n",
    "print(\"- Architecture: 4‚Üí16‚Üí1 (tanh activation)\")\n",
    "print(\"- Optimizer: Adam (lr=0.001)\")\n",
    "print(\"- Training: 20 epochs, batch_size=32\")\n",
    "print(\"- Random seed: 42\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Compare the results from individual training cells above\n",
    "print()\n",
    "print(\"Results:\")\n",
    "print(\"-\" * 60)\n",
    "print(f\"Scikit-learn:  Accuracy: {acc:.3f},  Time: {time_sklearn:.3f}s\")\n",
    "print(f\"PyTorch:       Accuracy: {acc_pytorch:.3f},  Time: {time_pytorch:.3f}s\")\n",
    "print(f\"PyTorch L.:    Accuracy: {acc_pl:.3f},  Time: {time_pl:.3f}s\")\n",
    "print(f\"Keras:         Accuracy: {acc_keras:.3f},  Time: {time_keras:.3f}s\")\n",
    "print(f\"JAX:           Accuracy: {acc_jax:.3f},  Time: {time_jax:.3f}s\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "print()\n",
    "print(\"‚úÖ All frameworks achieved similar accuracy (~0.98)\")\n",
    "print(\"üí° PyTorch Lightning is slower due to framework overhead\")\n",
    "print(\"üí° Pick based on ease of use, not performance differences!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comparison_summary",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Takeaway:\n",
    "\n",
    "All neural networks above use **identical setup**: \n",
    "- Architecture: 4‚Üí16‚Üí1 (tanh activation)\n",
    "- Optimizer: Adam (lr=0.001)\n",
    "- Training: 20 epochs\n",
    "- Batch size: 32\n",
    "- Same random seed (42)\n",
    "\n",
    "The comparison shows:\n",
    "- **Accuracy should are very similar** across neural network frameworks (differences reflect implementation details and numerical precision)\n",
    "- **Speed varies** due to different optimizations and backend implementations - also because this is a very small example \n",
    "\n",
    "**Bottom line:** Choose your framework based on ease of use and ecosystem, not tiny performance differences.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6747c527",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    " # 2 - ML workflow tools "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1de4eb",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "Moving beyond jupyter notebooks and into configuration file centred and reproducible ML "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b957acc7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 2.1 Logging experiments with Weights & Biases (W&B) \n",
    "\n",
    "**What it does:**\n",
    "- Automatic logging of metrics, hyperparameters, system info\n",
    "- Beautiful dashboards\n",
    "- Experiment comparison\n",
    "- Model versioning\n",
    "- Artifact tracking\n",
    "- **Free for academics!**\n",
    "\n",
    "**When to use:** Any serious project. Very easy to set up\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "61d4278a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/liv/pyhep-talk/venv-pyhep/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'repr' attribute with value False was provided to the `Field()` function, which has no effect in the context it was used. 'repr' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "/Users/liv/pyhep-talk/venv-pyhep/lib/python3.12/site-packages/pydantic/_internal/_generate_schema.py:2249: UnsupportedFieldAttributeWarning: The 'frozen' attribute with value True was provided to the `Field()` function, which has no effect in the context it was used. 'frozen' is field-specific metadata, and can only be attached to a model field using `Annotated` metadata or by assignment. This may have happened because an `Annotated` type alias using the `type` statement was used, or if the `Field()` function was attached to a single member of a union type.\n",
      "  warnings.warn(\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mlivv\u001b[0m (\u001b[33mlivv-CERN\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/liv/pyhep-talk/wandb/run-20251028_121812-jlsxh2ub</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/livv-CERN/pyhep-demo/runs/jlsxh2ub' target=\"_blank\">neural-network-comparison2</a></strong> to <a href='https://wandb.ai/livv-CERN/pyhep-demo' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/livv-CERN/pyhep-demo' target=\"_blank\">https://wandb.ai/livv-CERN/pyhep-demo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/livv-CERN/pyhep-demo/runs/jlsxh2ub' target=\"_blank\">https://wandb.ai/livv-CERN/pyhep-demo/runs/jlsxh2ub</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà</td></tr><tr><td>epoch</td><td>‚ñÅ‚ñÉ‚ñÖ‚ñÜ‚ñà</td></tr><tr><td>loss</td><td>‚ñà‚ñÑ‚ñÉ‚ñÇ‚ñÅ</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>accuracy</td><td>0.96</td></tr><tr><td>epoch</td><td>4</td></tr><tr><td>loss</td><td>0.26667</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">neural-network-comparison2</strong> at: <a href='https://wandb.ai/livv-CERN/pyhep-demo/runs/jlsxh2ub' target=\"_blank\">https://wandb.ai/livv-CERN/pyhep-demo/runs/jlsxh2ub</a><br> View project at: <a href='https://wandb.ai/livv-CERN/pyhep-demo' target=\"_blank\">https://wandb.ai/livv-CERN/pyhep-demo</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20251028_121812-jlsxh2ub/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé® W&B: Beautiful dashboards for experiment tracking\n"
     ]
    }
   ],
   "source": [
    "# W&B Quick Start - Experiment Monitoring\n",
    "\n",
    "import wandb\n",
    "#  # You need to create an account and log in the first time\n",
    "\n",
    "# Initialize tracking\n",
    "wandb.init(\n",
    "    project=\"pyhep-demo\", \n",
    "    name=\"neural-network-comparison2\",\n",
    "    config={\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"epochs\": 20,\n",
    "        \"batch_size\": 32,\n",
    "        \"architecture\": \"Scale‚Üí16‚Üí1\"\n",
    "    }\n",
    ")\n",
    "\n",
    "# Simulate training with fake metrics\n",
    "for epoch in range(5):\n",
    "    fake_loss = 1.0 / (epoch + 2) + 0.1\n",
    "    fake_acc = 0.8 + epoch * 0.04\n",
    "    wandb.log({\n",
    "        \"loss\": fake_loss,\n",
    "        \"accuracy\": fake_acc,\n",
    "        \"epoch\": epoch\n",
    "    })\n",
    "\n",
    "wandb.finish()\n",
    "print(\"üé® W&B: Beautiful dashboards for experiment tracking\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be5f73e2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 2.2 MLflow - The Open Source Alternative\n",
    "\n",
    "**Pros:**\n",
    "- Fully open source\n",
    "- Self-hosted (for the privacy-conscious)\n",
    "- Experiment tracking + model registry\n",
    "- Works with any ML library\n",
    "\n",
    "**Cons:**\n",
    "- Less pretty than W&B\n",
    "- Need to host it yourself\n",
    "- Smaller community\n",
    "\n",
    "**When to use:** You need full control, can't/won't use cloud services\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wandb_mlflow_comparison",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Key Difference: W&B vs MLflow\n",
    "\n",
    "**W&B (Weights & Biases):**\n",
    "- **Best at:** Experiment monitoring, visualization, hyperparameter tuning\n",
    "- Interactive dashboards, automatic logging\n",
    "- Great for research and experimentation\n",
    "- Cloud-first (free for academics)\n",
    "\n",
    "**MLflow:**\n",
    "-  **Best at:** Model registry, versioning, deployment, MLOps\n",
    "-  Model storage and retrieval\n",
    "-  Production deployment support\n",
    "-  On-premise friendly\n",
    "\n",
    "**TL;DR:** Use W&B for experiments, MLflow for production models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4145b4d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 2.3 Optuna - Hyperparameter Optimization Made Easy\n",
    "\n",
    "**Going beyond grid searches**\n",
    "\n",
    "Optuna uses smart algorithms (TPE, CMA-ES) to find good hyperparameters faster.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1dfad9ee",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-28 12:18:32,793] A new study created in memory with name: no-name-bb00ee5b-599b-4719-ad6b-6b2957044bc5\n",
      "[I 2025-10-28 12:18:32,805] Trial 0 finished with value: 0.8688109941757362 and parameters: {'lr': 0.0015138334581392512}. Best is trial 0 with value: 0.8688109941757362.\n",
      "[I 2025-10-28 12:18:32,806] Trial 1 finished with value: 0.9682190592135067 and parameters: {'lr': 3.0483825644801777e-05}. Best is trial 1 with value: 0.9682190592135067.\n",
      "[I 2025-10-28 12:18:32,807] Trial 2 finished with value: 0.8649559952782317 and parameters: {'lr': 0.014722167447595936}. Best is trial 1 with value: 0.9682190592135067.\n",
      "[I 2025-10-28 12:18:32,809] Trial 3 finished with value: 0.9640269403626934 and parameters: {'lr': 0.07898903711912038}. Best is trial 1 with value: 0.9682190592135067.\n",
      "[I 2025-10-28 12:18:32,809] Trial 4 finished with value: 0.9841493131744182 and parameters: {'lr': 0.02213876173293746}. Best is trial 4 with value: 0.9841493131744182.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'lr': 0.02213876173293746}\n",
      "Best value: 0.984\n",
      "üéØ Optuna: Smarter than grid search, easier than manual tuning\n",
      "üí° Integrates with W&B, PyTorch Lightning, etc.\n"
     ]
    }
   ],
   "source": [
    "# Optuna example - hyperparameter tuning\n",
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    # Suggest hyperparameters\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)\n",
    "    \n",
    "    # For demo, return a mock score based on lr\n",
    "    # In reality, you'd train a model with this lr and return validation score\n",
    "    import random\n",
    "    random.seed(int(lr * 1000))\n",
    "    accuracy = random.uniform(0.85, 0.99)\n",
    "    return accuracy\n",
    "\n",
    "# Create and run study\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=5, show_progress_bar=False)\n",
    "\n",
    "print(f\"Best params: {study.best_params}\")\n",
    "print(f\"Best value: {study.best_value:.3f}\")\n",
    "\n",
    "print(\"üéØ Optuna: Smarter than grid search, easier than manual tuning\")\n",
    "print(\"üí° Integrates with W&B, PyTorch Lightning, etc.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wandb_optuna_integration",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Combining W&B + Optuna: The Best of Both Worlds\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "wandb_optuna_code",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fp/5tqzs2jn7fgbdzfwyp_qjkmr0000gn/T/ipykernel_37954/1583329022.py:10: ExperimentalWarning: WeightsAndBiasesCallback is experimental (supported from v2.9.0). The interface can change in the future.\n",
      "  wandbc = WeightsAndBiasesCallback(metric_name=\"accuracy\", wandb_kwargs=wandb_kwargs)\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.22.2"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/liv/pyhep-talk/wandb/run-20251028_121845-5eku3h4a</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/livv-CERN/pyhep-optuna-demo/runs/5eku3h4a' target=\"_blank\">frosty-water-3</a></strong> to <a href='https://wandb.ai/livv-CERN/pyhep-optuna-demo' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/livv-CERN/pyhep-optuna-demo' target=\"_blank\">https://wandb.ai/livv-CERN/pyhep-optuna-demo</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/livv-CERN/pyhep-optuna-demo/runs/5eku3h4a' target=\"_blank\">https://wandb.ai/livv-CERN/pyhep-optuna-demo/runs/5eku3h4a</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-10-28 12:18:47,048] A new study created in memory with name: no-name-97826163-e908-4442-862b-54ce3d3ee275\n",
      "[I 2025-10-28 12:18:47,056] Trial 0 finished with value: 0.9682190592135067 and parameters: {'lr': 0.00012622735517145287, 'batch_size': 64}. Best is trial 0 with value: 0.9682190592135067.\n",
      "[I 2025-10-28 12:18:47,064] Trial 1 finished with value: 0.9682190592135067 and parameters: {'lr': 0.00045207508625450866, 'batch_size': 64}. Best is trial 0 with value: 0.9682190592135067.\n",
      "[I 2025-10-28 12:18:47,072] Trial 2 finished with value: 0.8817388203133468 and parameters: {'lr': 0.008723512491819385, 'batch_size': 64}. Best is trial 0 with value: 0.9682190592135067.\n",
      "[I 2025-10-28 12:18:47,077] Trial 3 finished with value: 0.9682190592135067 and parameters: {'lr': 0.00011168133878336508, 'batch_size': 16}. Best is trial 0 with value: 0.9682190592135067.\n",
      "[I 2025-10-28 12:18:47,080] Trial 4 finished with value: 0.9682190592135067 and parameters: {'lr': 7.837733635031695e-05, 'batch_size': 16}. Best is trial 0 with value: 0.9682190592135067.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Optuna trials automatically logged to W&B!\n",
      "Best params: {'lr': 0.00012622735517145287, 'batch_size': 64}\n",
      "\n",
      "üìä Check W&B dashboard: All trials with lr, batch_size, and accuracy\n"
     ]
    }
   ],
   "source": [
    "# W&B + Optuna Integration\n",
    "\n",
    "from optuna.integration.wandb import WeightsAndBiasesCallback\n",
    "    \n",
    "# Initialize W&B callback for Optuna\n",
    "wandb_kwargs = {\n",
    "    \"project\": \"pyhep-optuna-demo\",\n",
    "    \"job_type\": \"hyperparameter-optimization\"\n",
    "}\n",
    "wandbc = WeightsAndBiasesCallback(metric_name=\"accuracy\", wandb_kwargs=wandb_kwargs)\n",
    "\n",
    "def objective(trial):\n",
    "    # Optuna suggests hyperparameters\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64])\n",
    "    \n",
    "    # Mock training (in reality, train your model here)\n",
    "    import random\n",
    "    random.seed(int(lr * 1000))\n",
    "    accuracy = random.uniform(0.85, 0.99)\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "# Create study with W&B callback\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=5, callbacks=[wandbc], show_progress_bar=False)\n",
    "\n",
    "print(\"‚úÖ Optuna trials automatically logged to W&B!\")\n",
    "print(f\"Best params: {study.best_params}\")\n",
    "print(\"\\nüìä Check W&B dashboard: All trials with lr, batch_size, and accuracy\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cern_hep_tools",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 2.4 HEP-Specific ML Workflow Tools\n",
    "<div>\n",
    "<img src=\"images/b-hive-logo.png\" height=\"50\"/>\n",
    "</div>\n",
    "\n",
    "**b-hive** - CMS ML workflow tool \n",
    "- Codebase with links to documentation [here](https://gitlab.cern.ch/cms-btv/b-hive)\n",
    "- Was initially created for jet tagging, is now being made more general \n",
    "- Main use is ML for root files, also integrates awkward and uproot \n",
    "- CMS internal tool (so far)\n",
    "- Several similar tools in ATLAS, like salt, but they are more task specific, as far as I know \n",
    "\n",
    "<div>\n",
    "<img src=\"images/law-logo.png\" height=\"50\"/>\n",
    "</div>\n",
    "\n",
    "**LAW** - A tool for workflow orchestration\n",
    "- Documentation [here](https://law.readthedocs.io/en/latest/)\n",
    "- Mainly used by CMS  \n",
    "- Has abstractions for run locations and software environments \n",
    "- Integrates with e.g. HTCondor and CRAB \n",
    "\n",
    "\n",
    "**hep-ml-templates** - A new project for ML templating \n",
    "- Code [here](https://github.com/livaage/hep-ml-templates)\n",
    "- Aim is to be able to install entire generalised ML workflows with one step \n",
    "- Pick and choose between a large amount of modules containing best practices \n",
    "- Can quick start new projects or improve old ones \n",
    "- Started as an IRIS-HEP summer project - is a work in progress\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4818309",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 2.5 Industry workflow tools that we also use "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bce3c3b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Apache Airflow - Workflow Orchestration\n",
    "\n",
    "**What it is:** Platform to programmatically author, schedule, and monitor workflows\n",
    "\n",
    "**Pros:**\n",
    "- Visual workflow DAGs (directed acyclic graphs)\n",
    "- Scheduling and dependency management\n",
    "- Great UI for monitoring tasks\n",
    "- Python-based task definition\n",
    "- Industry standard for data pipelines\n",
    "\n",
    "**Cons:**\n",
    "- Requires infrastructure setup\n",
    "- Learning curve for DAG concepts\n",
    "- Can be complex for simple workflows\n",
    "\n",
    "**When to use:** You need to orchestrate complex data/ML pipelines with many steps\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2076a80",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Kubeflow - ML on Kubernetes\n",
    "\n",
    "**What it is:** Kubernetes-native platform for ML workflows\n",
    "\n",
    "**Pros:**\n",
    "- Containerized ML pipelines\n",
    "- Kubernetes orchestration (scale infinitely)\n",
    "- Built-in experiment tracking\n",
    "- Production-ready deployment\n",
    "- Reproducible ML workflows\n",
    "\n",
    "**Cons:**\n",
    "- Needs Kubernetes expertise\n",
    "- Infrastructure complexity\n",
    "- Steep learning curve\n",
    "\n",
    "**When to use:**\n",
    "- You need production ML at scale\n",
    "- Your org already uses Kubernetes\n",
    "- You need advanced ML pipeline orchestration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c633b5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Workflow Tools: Quick Comparison\n",
    "\n",
    "| Tool | Best For | Setup Difficulty | Cost |\n",
    "|------|----------|------------------|------|\n",
    "| **W&B** | Everything | Easy | Free (academic) |\n",
    "| **MLflow** | On-premise, privacy | Medium | Free (self-host) |\n",
    "| **Optuna** | Hyperparameter tuning | Easy | Free |\n",
    "| **b-hive** | CERN users | Easy | Free (CERN) |\n",
    "| **hep-ml-templates** | Quick project start | Easy | Free |\n",
    "| **LAW** | CERN workflows | Medium | Free (CERN) |\n",
    "| **Airflow** | Workflow orchestration | Hard | Free (self-host) |\n",
    "| **Kubeflow** | ML on Kubernetes | Hard | Free (self-host) |\n",
    "\n",
    "**Pro tip:** Use W&B + Optuna together. They integrate perfectly!\n",
    "**For CERN users:** b-hive is built-in, lower setup barrier!\n",
    "**For production:** Consider Airflow/Kubeflow if you need orchestration at scale\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74b38b8-b5f0-421b-b889-21ef300dfc03",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# 3. Model Training & Deployment\n",
    "\n",
    "Get off your laptop!\n",
    "\n",
    "Doing lots of ML locally is a quick way to burn through your hardware. \n",
    "The best option is usually a university cluster.\n",
    "\n",
    "---\n",
    "\n",
    "## 3.1 Tips for Working with a Grid System\n",
    "\n",
    "####  Monitor Hardware Utilization\n",
    "\n",
    "Always check if you're actually using the hardware efficiently:\n",
    "\n",
    "**SLURM systems:**\n",
    "```bash\n",
    "# After your job completes or while it's running\n",
    "jobstats <jobid>\n",
    "\n",
    "# Or check GPU utilization in real-time (if connected to node)\n",
    "nvidia-smi\n",
    "```\n",
    "\n",
    "**Pro tip:** If your GPU utilization is < 50%, you're probably not using the GPU effectively. Check for data loading bottlenecks!\n",
    "\n",
    "### SSH Port Forwarding for Jupyter Notebooks\n",
    "\n",
    "Even if your GPU cluster doesn't have wifi (they rarely do), you can still use Jupyter notebooks:\n",
    "\n",
    "**Step 1: SSH into the cluster with port forwarding**\n",
    "```bash\n",
    "# Forward local port 8888 to remote port 8888\n",
    "ssh -L 8888:localhost:8888 username@cluster.university.edu\n",
    "\n",
    "# If you're behind a login node, double forward:\n",
    "ssh -L 8888:compute-node:8888 username@login.cluster.edu\n",
    "```\n",
    "\n",
    "**Step 2: Start Jupyter on the cluster**\n",
    "```bash\n",
    "# After ssh'ing in, start jupyter\n",
    "jupyter lab --no-browser --port=8888\n",
    "# Or if you need a specific port:\n",
    "jupyter lab --no-browser --port=8888 --ip=0.0.0.0\n",
    "```\n",
    "\n",
    "**Step 3: Access from your laptop**\n",
    "- Open browser: `http://localhost:8888`\n",
    "- Copy the token from the cluster terminal\n",
    "- Paste it in the browser\n",
    "\n",
    "**Pro tip:** Use `screen` or `tmux` to keep sessions alive if your SSH connection drops!\n",
    "\n",
    "### SLURM Array Jobs for Hyperparameter Sweeps\n",
    "\n",
    "Run multiple jobs with different configurations efficiently:\n",
    "\n",
    "**Create a job script** (`train_job.sh`):\n",
    "```bash\n",
    "#!/bin/bash\n",
    "#SBATCH --job-name=ml_train\n",
    "#SBATCH --gres=gpu:1\n",
    "#SBATCH --time=04:00:00\n",
    "#SBATCH --mem=8G\n",
    "#SBATCH --array=0-4\n",
    "\n",
    "ARGS=(0.1 0.75 3 25.5 50)\n",
    "# Activate your environment\n",
    "source activate my_env\n",
    "\n",
    "# Run training with config based on array task ID\n",
    "python train.py --config configs/run_${SLURM_ARRAY_TASK_ID}.yaml\n",
    "```\n",
    "\n",
    "**Submit array job:**\n",
    "```bash\n",
    "# Submit 10 jobs with IDs 1-10\n",
    "sbatch --array=1-10 train_job.sh\n",
    "\n",
    "# Or specific indices\n",
    "sbatch --array=1,3,5-10 train_job.sh\n",
    "```\n",
    "\n",
    "### Resource Management Best Practices\n",
    "\n",
    "**Request appropriate resources:**\n",
    "```bash\n",
    "#SBATCH --gres=gpu:1          # One GPU is usually enough\n",
    "#SBATCH --cpus-per-task=4     # Match your data loader workers\n",
    "#SBATCH --mem=16G              # Request memory based on your batch size\n",
    "```\n",
    "\n",
    "**Use job dependencies for pipelines:**\n",
    "```bash\n",
    "# Submit data preprocessing job\n",
    "job1=$(sbatch preprocess.sh | awk '{print $4}')\n",
    "\n",
    "# Wait for it to complete, then train\n",
    "job2=$(sbatch --dependency=afterok:$job1 train.sh | awk '{print $4}')\n",
    "\n",
    "# Finally evaluate\n",
    "sbatch --dependency=afterok:$job2 evaluate.sh\n",
    "```\n",
    "\n",
    "### Data Transfer Optimization\n",
    "\n",
    "**Compress large datasets before transfer:**\n",
    "```bash\n",
    "# Cluster to local (compressed)\n",
    "ssh username@cluster \"tar -czf - data/\" | tar -xzf - -C ./local_data/\n",
    "\n",
    "# Or simpler: just use scp with compression\n",
    "scp -C -r username@cluster:~/data/ ./local_data/\n",
    "\n",
    "# Use rsync for incremental updates\n",
    "rsync -avz --progress username@cluster:~/data/ ./local_data/\n",
    "```\n",
    "\n",
    "### Environment Management\n",
    "\n",
    "**Create environment files that work across systems:**\n",
    "```yaml\n",
    "# environment.yml\n",
    "channels:\n",
    "  - pytorch\n",
    "  - conda-forge\n",
    "dependencies:\n",
    "  - python=3.10\n",
    "  - pytorch\n",
    "  - numpy\n",
    "  - pip\n",
    "  - pip:\n",
    "      - wandb\n",
    "```\n",
    "\n",
    "**Pro tip:** Use conda-pack to bundle your entire environment for easy transfer!\n",
    "\n",
    "```bash\n",
    "conda pack -n my_env -o my_env.tar.gz\n",
    "# Transfer and unpack on target machine\n",
    "tar -xzf my_env.tar.gz\n",
    "source my_env/bin/activate\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f029f30",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# 3.2 GPU Resources\n",
    "\n",
    "**All major HEP institutions provide GPU resources!**\n",
    "\n",
    "See detailed documentation: [CMS ML GPU Resources](https://cms-ml.github.io/documentation/Resources/GPU_Resources/index.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8058aed",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### GPU Resources at CERN \n",
    "\n",
    "\n",
    "Here's a comparison of GPU resources available at CERN (more details in the [CMS ML documentation](https://cms-ml.github.io/documentation/Resources/GPU_Resources/index.html)):\n",
    "\n",
    "| Resource | Access Level | Best For | Notes |\n",
    "|----------|--------------|----------|-------|\n",
    "| **SWAN** | CERN users | Quick prototyping, teaching | Web-based Jupyter, lxplus integration, pre-configured |\n",
    "| **lxplus-gpu.cern.ch** | CERN users | Interactive sessions | Direct GPU access, login node with GPUs |\n",
    "| **HTCondor** | CERN users | Large-scale training | Batch jobs, array submissions, queue system |\n",
    "| **ml.cern.ch** | CMS collaboration | Production ML | Dedicated ML infrastructure |\n",
    "\n",
    "HTCondor is the only one where you can get more than 1 GPU without asking \n",
    "\n",
    "\n",
    "**Quick tips:**\n",
    "- Start with **SWAN** for experimentation (easiest setup)\n",
    "- Use **lxplus-gpu** for interactive debugging\n",
    "- Submit **HTCondor** jobs for long training runs\n",
    "- Access **ml.cern.ch** for production workloads\n",
    "\n",
    "**Similar resources exist at other HEP institutions** - check with your local computing team!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1ac86b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 3.3 ONNX - Make your model portable\n",
    "\n",
    "**Problem:** Trained in PyTorch, but production uses TensorFlow (or C++, or...)\n",
    "\n",
    "**Solution:** ONNX (Open Neural Network Exchange)\n",
    "\n",
    "**What it does:**\n",
    "- Convert models between frameworks\n",
    "- Optimize for inference\n",
    "- Deploy anywhere (edge devices, web, etc.)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e5fa78",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1028 10:42:57.146000 28037 torch/onnx/_internal/exporter/_registration.py:107] torchvision is not installed. Skipping torchvision::nms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì¶ ONNX: Model exported and loaded successfully!\n",
      "Original prediction: 0.500\n",
      "ONNX prediction: 0.553\n",
      "\n",
      " ONNX: Train anywhere, deploy everywhere\n",
      "Especially useful for edge deployment and production\n"
     ]
    }
   ],
   "source": [
    "# ONNX Example - Export PyTorch model\n",
    "import torch.onnx\n",
    "\n",
    "# Create dummy input\n",
    "dummy_input = torch.randn(1, 4)\n",
    "\n",
    "# Export to ONNX\n",
    "torch.onnx.export(model_torch, dummy_input, \"model.onnx\", verbose=False)\n",
    "\n",
    "# Load and test with ONNX Runtime\n",
    "import onnxruntime as ort\n",
    "session = ort.InferenceSession(\"model.onnx\")\n",
    "\n",
    "# Test inference\n",
    "test_input = X_test[:1].astype('float32')\n",
    "ort_inputs = {session.get_inputs()[0].name: test_input}\n",
    "result = session.run(None, ort_inputs)\n",
    "\n",
    "print(\"üì¶ ONNX: Model exported and loaded successfully!\")\n",
    "print(f\"Original prediction: {model_torch(torch.from_numpy(X_train[:1]).float()).item():.3f}\")\n",
    "print(f\"ONNX prediction: {result[0][0][0]:.3f}\")\n",
    "\n",
    "print(\"\\n ONNX: Train anywhere, deploy everywhere\")\n",
    "print(\"Especially useful for edge deployment and production\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6080353",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 3.4 Sonic (NVIDIA Triton) - Production Inference Server\n",
    "\n",
    "**Problem:** You've trained a great model, now you need to serve it efficiently at scale\n",
    "\n",
    "**Solution:** [Sonic](https://fastmachinelearning.org/SuperSONIC/) - CERN's adaptation of NVIDIA Triton Inference Server\n",
    "\n",
    "**What it does:**\n",
    "- High-performance inference serving\n",
    "- Dynamic batching for throughput optimization\n",
    "- Multi-model concurrent inference\n",
    "- GPU acceleration with automatic batching\n",
    "- Model versioning and A/B testing\n",
    "- Easy integration with HEP analysis workflows\n",
    "\n",
    "**Key features for HEP:**\n",
    "- Handles multiple ONNX models efficiently\n",
    "- Optimized for batch inference (perfect for ROOT file processing)\n",
    "- Built for production CMS workflows\n",
    "- REST and gRPC APIs\n",
    "\n",
    "**When to use:**\n",
    "- Production inference at scale\n",
    "- Need concurrent model serving\n",
    "- Want dynamic batching optimization\n",
    "- Integrating ML into reconstruction/analysis pipelines\n",
    "\n",
    "**Similar to:** NVIDIA Triton (industry standard), TensorRT Inference Server\n",
    "\n",
    "**Pro tip:** Convert models to ONNX format, then serve with Sonic for optimal performance!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7471a7f2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 3.5 hls4ml - ML on FPGAs\n",
    "\n",
    "**When you really need speed**\n",
    "\n",
    "**Problem:** You need ultra-low latency inference (< 1 microsecond) for triggers\n",
    "\n",
    "**Solution:** hls4ml converts your neural network to FPGA firmware\n",
    "\n",
    "**Use cases:**\n",
    "- LHC trigger systems\n",
    "- Real-time event selection\n",
    "- Anything requiring hardware acceleration\n",
    "\n",
    "```python\n",
    "# import hls4ml\n",
    "# config = hls4ml.utils.config_from_keras_model(model, granularity='name')\n",
    "# hls_model = hls4ml.converters.convert_from_keras_model(\n",
    "# model, hls_config=config, output_dir='my-hls-test'\n",
    "# )\n",
    "# hls_model.compile()\n",
    "```\n",
    "Documentation [here](https://fastmachinelearning.org/hls4ml/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8fa14b-f23f-462e-bf46-a43031c639d6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# 4. HEP-ML Bridge Tools\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Preprocess vs. On-the-Fly: Which Strategy?\n",
    "\n",
    "An important decision in your ML workflow: **should you preprocess ROOT files and store the results, or read directly during training?**\n",
    "\n",
    "| Strategy | When to Use | Pros | Cons |\n",
    "|----------|-------------|------|------|\n",
    "| **Read directly from ROOT** | Small datasets (< 10GB), prototyping, frequently changing processing | Simple workflow, less storage, easy to iterate on preprocessing | Slow I/O, hard to reproduce exactly, network issues can cause crashes |\n",
    "| **Preprocess & store** | Large datasets (> 10GB), production training, multiple experiments | Fast training, reproducible, GPU stays busy, efficient data loading | Extra storage needed, preprocessing step adds complexity |\n",
    "| **Hybrid: Cache processed batches** | Medium datasets, uncertain preprocessing | Best of both worlds - fast after first epoch | Still need storage, cache management complexity |\n",
    "\n",
    "**Recommendation:** \n",
    "- **Prototyping:** Read directly from ROOT (simpler, faster iteration)\n",
    "- **Production:** Preprocess to numpy/hdf5/parquet (faster, more reliable)\n",
    "- **Large scale:** Use awkward arrays, uproot, and preprocessing pipeline\n",
    "\n",
    "**These tools save your sanity:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28845a9e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### uproot - Read ROOT files without ROOT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41985ec8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üéâ uproot: Read ROOT-like data successfully!\n",
      "Events: 1000, Columns: ['jet_pt', 'jet_eta', 'jet_phi']\n",
      "\n",
      "Sample data:\n",
      "       jet_pt   jet_eta   jet_phi\n",
      "0   20.508263 -1.316974  0.257014\n",
      "1  153.028610 -1.240321  1.234205\n",
      "2   75.658781 -0.339718  1.270121\n",
      "3   49.887287  0.551048 -2.062808\n",
      "4    9.521251  1.370377  0.000708\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/fp/5tqzs2jn7fgbdzfwyp_qjkmr0000gn/T/ipykernel_28037/1353083796.py:15: FutureWarning: Starting in version 5.7.0, Uproot will default to writing RNTuples instead of TTrees. You will need to use `mktree` to explicitly create a TTree. Please update your code accordingly.\n",
      "  file[\"Events\"] = {\n"
     ]
    }
   ],
   "source": [
    "# uproot example - create mock HEP data\n",
    "import uproot\n",
    "import numpy as np\n",
    "from hist import Hist\n",
    "\n",
    "# Create a mock ROOT file with HEP-like data\n",
    "with uproot.recreate(\"mock_data.root\") as file:\n",
    "    # Simulate jet pt distribution\n",
    "    np.random.seed(42)\n",
    "    jet_pt = np.random.exponential(50, size=1000) * np.random.uniform(0.8, 1.2, size=1000)\n",
    "    jet_eta = np.random.normal(0, 1.5, size=1000)\n",
    "    jet_phi = np.random.uniform(-np.pi, np.pi, size=1000)\n",
    "    \n",
    "    # Create branches\n",
    "    file[\"Events\"] = {\n",
    "        \"jet_pt\": jet_pt,\n",
    "        \"jet_eta\": jet_eta,\n",
    "        \"jet_phi\": jet_phi,\n",
    "    }\n",
    "\n",
    "# Now read it back with uproot\n",
    "file = uproot.open(\"mock_data.root\")\n",
    "tree = file[\"Events\"]\n",
    "\n",
    "# Get branches as arrays\n",
    "pt = tree[\"jet_pt\"].array()\n",
    "eta = tree[\"jet_eta\"].array()\n",
    "\n",
    "# Or as pandas DataFrame\n",
    "df = tree.arrays([\"jet_pt\", \"jet_eta\", \"jet_phi\"], library=\"pd\")\n",
    "\n",
    "print(\"üéâ uproot: Read ROOT-like data successfully!\")\n",
    "print(f\"Events: {len(df)}, Columns: {list(df.columns)}\")\n",
    "print(f\"\\nSample data:\\n{df.head()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3a15c30",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Awkward Array - Handle Jagged Data\n",
    "\n",
    "**Problem:** HEP events have variable-length lists (jets, tracks, etc.)\n",
    "\n",
    "**Standard approach:** Pad everything, waste memory, write ugly code\n",
    "\n",
    "**Awkward Array:** Numpy for jagged/nested/variable-length data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ffbf10d9",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jet pts: [[50, 30], [100], [40, 35, 25]]\n",
      "Leading jet pt per event: [50, 100, 40]\n",
      "High-pt jets: [[{pt: 50, eta: 0.1}], [{pt: 100, eta: 1.2}], [{pt: 40, eta: 0.3}]]\n",
      "\n",
      "‚ú® Awkward: No more padding! No more for-loops!\n"
     ]
    }
   ],
   "source": [
    "import awkward as ak\n",
    "\n",
    "# Events with variable numbers of jets\n",
    "events = ak.Array([\n",
    "    {\"jets\": [{\"pt\": 50, \"eta\": 0.1}, {\"pt\": 30, \"eta\": -0.5}]},  # 2 jets\n",
    "    {\"jets\": [{\"pt\": 100, \"eta\": 1.2}]},                           # 1 jet\n",
    "    {\"jets\": [{\"pt\": 40, \"eta\": 0.3}, {\"pt\": 35, \"eta\": 0.8}, {\"pt\": 25, \"eta\": -1.0}]}  # 3 jets\n",
    "])\n",
    "\n",
    "# Operations work naturally on jagged data!\n",
    "jet_pts = events.jets.pt\n",
    "print(\"Jet pts:\", jet_pts)\n",
    "\n",
    "# Calculate things per event\n",
    "leading_jet_pt = ak.max(events.jets.pt, axis=1)\n",
    "print(\"Leading jet pt per event:\", leading_jet_pt)\n",
    "\n",
    "# Slice like numpy\n",
    "high_pt_jets = events.jets[events.jets.pt > 35]\n",
    "print(\"High-pt jets:\", high_pt_jets)\n",
    "\n",
    "print(\"\\n‚ú® Awkward: No more padding! No more for-loops!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40dbdec7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### hist - Modern Histogramming\n",
    "\n",
    "**ROOT's TH1/TH2 are... showing their age.**\n",
    "\n",
    "`hist` is a modern, Pythonic histogramming library:\n",
    "- Clean syntax\n",
    "- Integrates with numpy, awkward\n",
    "- Beautiful plotting with matplotlib/mplhep\n",
    "- Type hints, named axes, units\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5888e3ea",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä hist: Histogram created and saved!\n",
      "\n",
      "üí° Named axes, units, better plotting. Just better.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Modern histogramming with hist\n",
    "from hist import Hist\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Create histogram with named axes\n",
    "np.random.seed(42)\n",
    "pt_data = np.random.exponential(50, 1000) * np.random.uniform(0.8, 1.2, 1000)\n",
    "\n",
    "h = Hist.new.Reg(50, 0, 200, name=\"pt\", label=\"$p_T$ [GeV]\").Double()\n",
    "h.fill(pt=pt_data)\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "h.plot1d(ax=ax)\n",
    "ax.set_xlabel(\"Jet $p_T$ [GeV]\")\n",
    "ax.set_ylabel(\"Events\")\n",
    "ax.set_title(\"Mock Jet $p_T$ Distribution\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"jet_pt_hist.png\", dpi=100, bbox_inches='tight')\n",
    "print(\"üìä hist: Histogram created and saved!\")\n",
    "\n",
    "print(\"\\nüí° Named axes, units, better plotting. Just better.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6089f22a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 5. Industry Tools\n",
    "\n",
    "## What industry does better (and what we can steal)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5172abf4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Testing & Linting - It is worth it \n",
    "\n",
    "**Industry:** Comprehensive tests, CI/CD, code review, linting \n",
    "**HEP:** \"It worked on my machine\" \n",
    "\n",
    "**Tools you should strongly consider using:**\n",
    "\n",
    "1. **pytest** - Testing framework\n",
    "2. **black** - Code formatter \n",
    "3. **ruff** - Fast linter\n",
    "4. **Type hinting** - Make sure use cases are clear\n",
    "5. **pre-commit** - Run checks before committing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b890eb7d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running tests...\n",
      "‚úÖ All tests passed!\n",
      "\n",
      "üí° Pro tip: Test your preprocessing! That's where most bugs hide.\n"
     ]
    }
   ],
   "source": [
    "# Quick testing example - run simple tests\n",
    "import pytest\n",
    "\n",
    "def test_model_output_shape():\n",
    "    model = SimpleNN()\n",
    "    x = torch.randn(10, 4)\n",
    "    output = model(x)\n",
    "    assert output.shape == (10, 1), \"Wrong output shape!\"\n",
    "    return True\n",
    "\n",
    "def test_model_output_range():\n",
    "    model = SimpleNN()\n",
    "    x = torch.randn(10, 4)\n",
    "    output = model(x)\n",
    "    assert torch.all(output >= 0) and torch.all(output <= 1), \"Sigmoid broken!\"\n",
    "    return True\n",
    "\n",
    "# Run tests\n",
    "print(\"Running tests...\")\n",
    "try:\n",
    "    test_model_output_shape()\n",
    "    test_model_output_range()\n",
    "    print(\"‚úÖ All tests passed!\")\n",
    "except AssertionError as e:\n",
    "    print(f\"‚ùå Test failed: {e}\")\n",
    "\n",
    "print(\"\\nüí° Pro tip: Test your preprocessing! That's where most bugs hide.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f45416b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### GitHub Actions - Automate Everything\n",
    "\n",
    "Github actions can automatically run tests, including running automated physics plots! \n",
    "\n",
    "Example `.github/workflows/test.yml`:\n",
    "```yaml\n",
    "name: Tests\n",
    "on: [push, pull_request]\n",
    "jobs:\n",
    " test:\n",
    " runs-on: ubuntu-latest\n",
    " steps:\n",
    " - uses: actions/checkout@v3\n",
    " - uses: actions/setup-python@v4\n",
    " with:\n",
    " python-version: '3.10'\n",
    " - run: pip install -r requirements.txt\n",
    " - run: pytest\n",
    " - run: ruff check .\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e966352",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### An example use case from jet tagging at L1T - full talk here [https://indico.cern.ch/event/1496673/contributions/6637937/]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b946a963",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "<div>\n",
    "<img src=\"images/githubaction_example.png\" height=\"400\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629f2e4f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### AWS SageMaker - When You Need Industrial Scale\n",
    "\n",
    "**What it is:** AWS's ML platform (training, deployment, everything)\n",
    "\n",
    "**Pros:**\n",
    "- Scales to infinity\n",
    "- Managed infrastructure\n",
    "- Production-ready deployment\n",
    "- AutoML features\n",
    "\n",
    "**Cons:**\n",
    "- Costs money (sometimes a lot)\n",
    "- Learning curve\n",
    "- Vendor lock-in\n",
    "\n",
    "**When to use:** \n",
    "- You need serious scale\n",
    "- You have budget\n",
    "- Production deployment\n",
    "\n",
    "**HEP alternative:** Usually HTCondor + custom scripts (cheaper, less polished)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5473c5e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### What HEP Can Learn from Industry\n",
    "\n",
    "| Practice | Industry | HEP | What to do |\n",
    "|----------|----------|-----|-----------|\n",
    "| **Testing** | Comprehensive | Sparse | Write pytest tests! |\n",
    "| **CI/CD** | GitHub Actions | Manual | Add GitHub Actions |\n",
    "| **Code Review** | Required | Optional | Make PRs mandatory |\n",
    "| **Documentation** | Detailed | \"See code\" | Write docstrings |\n",
    "| **Versioning** | Semantic | Git SHA | Use proper versions |\n",
    "| **Linting** | Enforced | What's that? | Use ruff/black |\n",
    "\n",
    "**Bottom line:** Treat your code like a product, not a script.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8691d10",
   "metadata": {},
   "source": [
    "If you are looking to transition from academia to industry in ML, one of the key things they might want that we don't get much in HEP is AWS and SQL. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ec2a2b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# 6. Fun Shortcuts & \"Cheats\"\n",
    "\n",
    "The secret sauce. Ways to make it seem like you've been working hard when you haven't"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec254bce",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 6.1. Make LLMs Do Your Work\n",
    "\n",
    "**Good uses:**\n",
    "- Boilerplate code (data loaders, training loops)\n",
    "- Documentation and docstrings\n",
    "- Code explanation\n",
    "- Unit test generation\n",
    "- Fixing linting errors \n",
    "\n",
    "**Bad uses:**\n",
    "- Can be bad at bug fixing \n",
    "- The tests it creates can be meaningless \n",
    "- It won't be very useful in knowing how to pre-process your features or choosing model architectures\n",
    "- It can't to hyperparameter optimization for you  \n",
    "\n",
    "**Pro tips:**\n",
    "- Be specific\n",
    "- **Always understand the code it gives you**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a83e18d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 6.2. Steal from Hugging Face\n",
    "\n",
    "**Hugging Face:** GitHub for ML models\n",
    "\n",
    "**What's there:**\n",
    "- 500,000+ pre-trained models\n",
    "- Datasets\n",
    "- Code examples\n",
    "- Entire pipelines\n",
    "\n",
    "**You can:**\n",
    "- Fine-tune existing models (faster than training from scratch)\n",
    "- Use pre-trained embeddings\n",
    "- Copy architectures\n",
    "- Download datasets\n",
    "\n",
    "\n",
    "\n",
    "Because of a package conflict I don't run examples here, but check out the [transformers library](https://github.com/huggingface/transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7335c43",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Example: Searching for sentiment analysis models\n",
      "------------------------------------------------------------\n",
      "1. tabularisai/multilingual-sentiment-analysis\n",
      "   Downloads: 457631\n",
      "2. lxyuan/distilbert-base-multilingual-cased-sentiments-student\n",
      "   Downloads: 629858\n",
      "3. cardiffnlp/twitter-roberta-base-sentiment-latest\n",
      "   Downloads: 4310462\n",
      "\n",
      "‚úÖ Browse 500,000+ models without downloading!\n",
      "üí° Visit: https://huggingface.co/models\n"
     ]
    }
   ],
   "source": [
    "# Hugging Face Hub: Browse and use models\n",
    "\n",
    "from huggingface_hub import list_models, hf_hub_download\n",
    "\n",
    "# List models - no download required, just metadata\n",
    "print(\"üîç Example: Searching for sentiment analysis models\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "models = list(list_models(search=\"sentiment\", task=\"text-classification\", limit=3))\n",
    "for i, model in enumerate(models, 1):\n",
    "    print(f\"{i}. {model.id}\")\n",
    "    print(f\"   Downloads: {model.downloads}\")\n",
    "\n",
    "print(\"\\n‚úÖ Browse 500,000+ models without downloading!\")\n",
    "print(\"üí° Visit: https://huggingface.co/models\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361d097b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Quick Prototyping Tricks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf62060b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 6.3. Prototyping with fastai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "e8fee29e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.338128</td>\n",
       "      <td>0.326519</td>\n",
       "      <td>0.515000</td>\n",
       "      <td>00:02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.246069</td>\n",
       "      <td>0.212036</td>\n",
       "      <td>0.515000</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.200686</td>\n",
       "      <td>0.135053</td>\n",
       "      <td>0.515000</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.162660</td>\n",
       "      <td>0.087033</td>\n",
       "      <td>0.515000</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.136544</td>\n",
       "      <td>0.072431</td>\n",
       "      <td>0.515000</td>\n",
       "      <td>00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fastai: Prototype in minutes, not hours!\n",
      "Great for quick experiments and learning\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Trick 5: fastai for rapid prototyping\n",
    "\n",
    "from fastai.tabular.all import *\n",
    "import pandas as pd\n",
    "\n",
    "# Create dataset from your features\n",
    "data = pd.DataFrame(X_train, columns=[f'feature_{j}' for j in range(4)])\n",
    "data['target'] = y_train\n",
    "\n",
    "# Use fastai - very simple!\n",
    "dls = TabularDataLoaders.from_df(data, path='.', y_names='target')\n",
    "learn = tabular_learner(dls, layers=[200,100], metrics=accuracy)\n",
    "\n",
    "learn.fit(5, lr=1e-3)\n",
    "\n",
    "print('fastai: Prototype in minutes, not hours!')\n",
    "print('Great for quick experiments and learning')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "808aec4f",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Summary:\n",
      "\n",
      "Model architecture (repr):\n",
      "SimpleNN(\n",
      "  (layers): Sequential(\n",
      "    (0): Linear(in_features=4, out_features=16, bias=True)\n",
      "    (1): Tanh()\n",
      "    (2): Linear(in_features=16, out_features=1, bias=True)\n",
      "    (3): Sigmoid()\n",
      "  )\n",
      ")\n",
      "Took 0.000s\n"
     ]
    }
   ],
   "source": [
    "# Quick tricks demo\n",
    "\n",
    "# Trick: torchinfo for model summary\n",
    "from torchinfo import summary\n",
    "\n",
    "# Show summary of our PyTorch model\n",
    "print(\"Model Summary:\")\n",
    "summary(SimpleNN(), input_size=(32, 4))  # batch_size=32, features=4\n",
    "\n",
    "# Trick: Use repr to see object details\n",
    "print(\"\\nModel architecture (repr):\")\n",
    "print(SimpleNN())\n",
    "\n",
    "# Trick: Quick timing\n",
    "import time\n",
    "start = time.time()\n",
    "# ... your code ...\n",
    "print(f\"Took {time.time() - start:.3f}s\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d053aa",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 6.4. Dataset Shortcuts\n",
    "\n",
    "**Don't start from scratch:**\n",
    "\n",
    "1. **Papers with Code** - Find datasets and benchmarks\n",
    "2. **Kaggle** - Tons of curated datasets\n",
    "3. **UCI ML Repository** - Classic datasets\n",
    "4. **HEP Data** - Published HEP datasets\n",
    "5. **Zenodo** - Open science data\n",
    "\n",
    "\n",
    "**Pro tip:** Start with a small subset! Debug on 1000 events, not 1M.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa8867b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 6.5. The Ultimate Shortcut List\n",
    "\n",
    "**Good resources:**\n",
    "\n",
    " **Learning:**\n",
    "- fast.ai course (free, excellent)\n",
    "- PyTorch tutorials (official)\n",
    "- Papers with Code (implementations)\n",
    "\n",
    "üõ†Ô∏è **Tools:**\n",
    "- GitHub Copilot / Cursor (AI pair programmer)\n",
    "- Paperswithcode.com (find state-of-the-art)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d99d0378",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary: Your ML Toolkit\n",
    "\n",
    "## Quick Reference Guide\n",
    "\n",
    "### For Beginners:\n",
    "1. **Start here:** Scikit-learn for classical ML, Keras for deep learning\n",
    "2. **Read data:** uproot for ROOT files\n",
    "3. **Track experiments:** W&B (free for academics!)\n",
    "4. **Learn:** fast.ai course, official PyTorch tutorials\n",
    "\n",
    "### For Intermediate Users:\n",
    "1. **Framework:** PyTorch or PyTorch Lightning\n",
    "2. **Data:** uproot + awkward array\n",
    "3. **Optimization:** Optuna\n",
    "4. **Deployment:** ONNX\n",
    "5. **Code quality:** pytest, ruff, GitHub Actions\n",
    "\n",
    "### For Advanced Users:\n",
    "1. **Speed:** JAX for compute-intensive tasks\n",
    "2. **Scale:** HTCondor or cloud \n",
    "3. **Hardware:** Optimise your GPU usage better\n",
    "4. **Tools:** Custom pipelines with all the above\n",
    "\n",
    "### Universal Tips:\n",
    "- Use version control (git)\n",
    "- Write tests (pytest)\n",
    "- Log experiments (W&B/MLflow)\n",
    "- Document your code\n",
    "- Start small, scale up\n",
    "- Leverage pre-trained models (Hugging Face)\n",
    "- Use LLMs wisely (verify everything!)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf736e8b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Final Thoughts\n",
    "\n",
    "## The ML landscape is vast, but you don't need to know everything\n",
    "\n",
    "**Key takeaways:**\n",
    "\n",
    "1. **Pick tools that fit YOUR needs** - Don't use fancy tools just because they're fancy\n",
    "2. **Start simple, add complexity** - Scikit-learn ‚Üí PyTorch ‚Üí JAX\n",
    "3. **Steal shamelessly** - Use pre-trained models, copy good code, ask LLMs\n",
    "4. **Automate early** - W&B, GitHub Actions, testing save time in the long run\n",
    "5. **Bridge HEP ‚Üî ML** - uproot, awkward, hist make life easier\n",
    "6. **Learn from industry** - Testing, CI/CD, code quality matter\n",
    "7. **Community is key** - Consider contributing on an opensource ML pacakge\n",
    "\n",
    "---\n",
    "\n",
    "## Most important:\n",
    "\n",
    "### **The best tool is the one you'll actually use.**\n",
    "\n",
    "Perfect code that doesn't exist < Working code that's \"good enough\"\n",
    "\n",
    "---\n",
    "\n",
    "# Questions? \n",
    "\n",
    "Resources:\n",
    "- These slides: https://github.com/livaage/pyhep-up-your-ml-game\n",
    "- My contact: liv.helen.vage@cern.ch\n",
    "\n",
    "**Now go build something cool!** \n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  },
  "rise": {
   "_rise_js_custom": "rise_scroll.js",
   "autoAdvance": false,
   "autoPlayMedia": true,
   "autoSlide": 0,
   "autolaunch": false,
   "center": false,
   "controls": true,
   "controlsBackArrows": "visible",
   "custom_css": "rise.css",
   "enable_chalkboard": true,
   "fragmentInURL": false,
   "fragments": false,
   "height": "100%",
   "hideAddressBar": true,
   "hideInactiveCursor": false,
   "history": true,
   "keyboard": true,
   "margin": 0,
   "maxScale": 1.5,
   "minScale": 0.2,
   "pdfSeparateFragments": false,
   "previewLinks": false,
   "progress": true,
   "scroll": false,
   "slideNumber": true,
   "start_slideshow_at": "selected",
   "theme": "night",
   "transition": "slide",
   "transitionSpeed": "slow",
   "viewDistance": 3,
   "width": "100%"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
